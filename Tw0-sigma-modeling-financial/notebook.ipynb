{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "##Exploring several ways to understand the data##",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in\n\nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns\nimport itertools\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "train.h5\n\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import kagglegym\n\nenv = kagglegym.make()\n\n# get the reference to the api\nobservation = env.reset()",
      "execution_count": 2,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "with pd.HDFStore('../input/train.h5') as train:\n    df = train.get('train')",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Opening ../input/train.h5 in read-only mode\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train = df.copy(deep = True)\nID = \"id\"\nTSTAMP = \"timestamp\"\nTARGET = \"y\"\nprint(\"There are {} rows and {} columns in the dataset\".format(*train.shape))",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "There are 1710756 rows and 111 columns in the dataset\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " **Let us divide the columns according to the prefixes**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def findMatchedColumnsUsingPrefix(prefix, df):\n    columns = df.columns[df.columns.str.startswith(prefix)]\n    return list(columns.values)",
      "execution_count": 5,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "derived_columns = findMatchedColumnsUsingPrefix(\"derived\", train)\nfundamental_columns = findMatchedColumnsUsingPrefix(\"fundamental\", train)\ntechnical_columns = findMatchedColumnsUsingPrefix(\"technical\", train)\n\nprint(\"There are {} derived columns\".format(len(derived_columns)))\nprint(\"There are {} fundamental columns\".format(len(fundamental_columns)))\nprint(\"There are {} technical columns\".format(len(technical_columns)))",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "There are 5 derived columns\nThere are 63 fundamental columns\nThere are 40 technical columns\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ids = train[ID].unique()\ntstamps = train[TSTAMP].unique()",
      "execution_count": 7,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**This function calculates the number of missing records within an asset for the group of columns passes as a reference**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def findMissingEntriesForIds(columns, train, identifier):\n    indexes = []\n    \n    # prepare the header\n    rows = {}\n    rows['id'] = []\n    for column in columns:\n        rows[column] = []\n        \n    # count number of missing entries in a column for an id group\n    for id, group in train.groupby(identifier):\n        rows['id'].append(id)\n        for column in columns:\n            rows[column].append(pd.isnull(group[column]).sum())\n            \n    df = pd.DataFrame(rows)\n    #df.columns = pd.MultiIndex.from_tuples([tuple(c.split('_')) for c in df.columns])\n    #df = df.stack(0).reset_index(1)\n    #df.sort_index()\n    return df",
      "execution_count": 8,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Calculates count of missing records for derived columns group by assets**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "derived = findMissingEntriesForIds(derived_columns, train, ID)\nderived.head(2)",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>derived_0</th>\n      <th>derived_1</th>\n      <th>derived_2</th>\n      <th>derived_3</th>\n      <th>derived_4</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   derived_0  derived_1  derived_2  derived_3  derived_4  id\n0          3          3          3          3          3   0\n1          4          4          4          4          4   6"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Calculates count of missing records for fundamental columns group by assets**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fundamental = findMissingEntriesForIds(fundamental_columns, train, ID)\nfundamental.head(2)",
      "execution_count": 10,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Calculates count of missing records for technical columns group by assets**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "technical = findMissingEntriesForIds(technical_columns, train, ID)\ntechnical.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**This function creates a dataset of those columns which have missing records**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def calculateColumnSum(res, columns):\n    names = []\n    values = []\n    for column in columns:\n        names.append(column)\n        values.append(res[column].sum())\n    data = pd.DataFrame({'columns' : names, 'counts' : values})\n    data = data.sort_values(by=['counts'], ascending = False)\n    return data",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def createHorizontalBarPlot(labels, missing, plot_width, plot_height, width, title):\n    N = len(labels)\n    ind = np.arange(N)\n    fig, ax = plt.subplots(figsize = (plot_width, plot_height))\n    rects = ax.barh(ind, missing, width)\n    ax.set_yticks(ind + width / 2)\n    ax.set_yticklabels(labels)\n    ax.set_title(title)\n    plt.show()\n    plt.close()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "result = calculateColumnSum(derived, derived_columns)\nprint(\"The columns with maximum null is {}\".format(result.iloc[0]['columns']))\nprint(\"The columns with minimum null is {}\".format(result.iloc[-1]['columns']))\n\ncreateHorizontalBarPlot(result['columns'], result['counts'], 7, 6, 0.45, 'Missing counts by columns')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "result = calculateColumnSum(fundamental, fundamental_columns)\nprint(\"The columns with maximum null is {}\".format(result.iloc[0]['columns']))\nprint(\"The columns with minimum null is {}\".format(result.iloc[-1]['columns']))\n\ncreateHorizontalBarPlot(result['columns'], result['counts'], 10, 15, 0.65, 'Missing counts by columns')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "result = calculateColumnSum(technical, technical_columns)\nprint(\"The columns with maximum null is {}\".format(result.iloc[0]['columns']))\nprint(\"The columns with minimum null is {}\".format(result.iloc[-1]['columns']))\n\ncreateHorizontalBarPlot(result['columns'], result['counts'], 10, 15, 0.45, 'Missing counts for technicals')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Join the frames of missing records counts for a column group by id for all columns**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "merged = derived.merge(fundamental, how = 'inner', on = 'id')\nmerged = merged.merge(technical, how = 'inner', on = 'id')\nmerged.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Add a new column sum which calculates count of missing records for an asset across all columns**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "copy = merged.copy(deep = True)\ncopy['sum'] = copy.apply(lambda x : x.sum(), axis = 1)\ncopy = copy.sort_values(by='sum', ascending = True)\nresult = copy[['id', 'sum']]\nresult.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"The asset with maximum missing records for all columns = {}\".format(result.iloc[-1][ID]))\n\nprint(\"The asset with minimum missing records for all columns = {}\".format(result.iloc[0][ID]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**This chart prints the top 10 asset ids with maximum missing records**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plot_df = result[:5]\ncreateHorizontalBarPlot(plot_df['id'], plot_df['sum'], 5, 4, 0.75, 'Missing by assets')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plot_df = result.tail(5)\ncreateHorizontalBarPlot(plot_df['id'], plot_df['sum'], 5, 4, 0.75, 'Missing by assets')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**This function calculates the timestamps where data is missing for an asset**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def getTstampForMissingRecordsInColumn(df, columns, identifier):\n    records = []\n    for id, group in df.groupby(identifier):\n        group_index_dict = {}\n        group_index_dict[identifier] = id\n        for col in columns:\n            group_index_dict[col] = list(group[pd.isnull(group[col])][TSTAMP])\n        records.append(group_index_dict)\n    return records",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "derived_missing_tstamps = getTstampForMissingRecordsInColumn(train, derived_columns, ID)\nderived_missing_tstamps[:1]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**From the result of the above function we calculate the difference between the maximum and the minimum timestamp to determine if they are consecutive in nature**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def findTstampDiffForMissingRecordsByID(tstamp_indices):\n    rows = []\n    for item in tstamp_indices:\n        row_dict = {}\n        for key, value in item.items():\n            if key == 'id':\n                row_dict[key] = value\n            else:\n                row_dict[key] = int((value[-1] - value[0] + 1) / len(value)) if len(value) > 1 else len(value)\n        rows.append(row_dict)\n\n    return pd.DataFrame(rows)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tstamp_diff_df = findTstampDiffForMissingRecordsByID(derived_missing_tstamps)\ntstamp_diff_df.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***1* - The timestamps for the missing entries are in succession.**\n***0* - There are no missing records.**\n***Anything besides 1 and 0 means that the timestamps are not in succession, some entries are there between the missing values.***",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**This function brings out those columns with timestamps having missing records but are not consecutive**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def findColumnsWithDiscreteMissingTstampDiff(columns, tstamp_diff_dataframe):\n    columns_with_discrete_missing_tstamps = {}\n    \n    for column in columns:\n        unique_arr = list(tstamp_diff_dataframe[column].unique())\n        temp = [i for i in unique_arr if i not in [1, 0]]\n        if len(temp) > 0:\n            columns_with_discrete_missing_tstamps[column] = temp\n        \n    return columns_with_discrete_missing_tstamps",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tstamp_diff_dict = findColumnsWithDiscreteMissingTstampDiff(derived_columns, tstamp_diff_df)\ntstamp_diff_dict",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Determine the identifiers where the missing timestamps are not in succession**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def findAssetsWithDiscreteMissingTstamp(discrete_missing_tstamp_diff_dict, tstamp_diff_data):\n    assets_with_discrete_missing_tstamps = []\n    \n    for key, values in discrete_missing_tstamp_diff_dict.items():\n        data = tstamp_diff_data.ix[tstamp_diff_data[key].isin(values)]\n        assets_with_discrete_missing_tstamps += list(data[ID].values)\n    \n    return list(set(assets_with_discrete_missing_tstamps))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "id_with_discrete_missing_tstamp = findAssetsWithDiscreteMissingTstamp(tstamp_diff_dict, tstamp_diff_df)\nid_with_discrete_missing_tstamp",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fund_column_indices = getTstampForMissingRecordsInColumn(train, fundamental_columns, ID)\ntstamp_diff_df = findTstampDiffForMissingRecordsByID(fund_column_indices)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tstamp_dict = findColumnsWithDiscreteMissingTstampDiff(fundamental_columns, tstamp_diff_df)\nid_with_discrete_missing_tstamp += findAssetsWithDiscreteMissingTstamp(tstamp_dict, tstamp_diff_df)\nid_with_discrete_missing_tstamp = list(set(id_with_discrete_missing_tstamp))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tech_column_indices = getTstampForMissingRecordsInColumn(train, technical_columns, ID)\ntstamp_diff_df = findTstampDiffForMissingRecordsByID(tech_column_indices)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tstamp_dict = findColumnsWithDiscreteMissingTstampDiff(technical_columns, tstamp_diff_df)\nid_with_discrete_missing_tstamp += findAssetsWithDiscreteMissingTstamp(tstamp_dict, tstamp_diff_df)\nid_with_discrete_missing_tstamp = list(set(id_with_discrete_missing_tstamp))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"The assets with missing entries that are not consecutive is {}\".format(id_with_discrete_missing_tstamp))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def barplot(res, width, index, column):\n    N = int(res.shape[0])\n    fig, ax = plt.subplots(figsize = (2, 1))\n    ind = np.arange(N)\n    rects = plt.bar(res['diff'], res['size'])\n    ax.set_title(column)\n    ax.set_xticks(ind + width / 2)\n    ax.set_xticklabels(res['diff'])\n    plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " **##Calculate the statistics for each derived column for each asset ##** ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def calculate_statistics(data, columns, identifier, excludes):\n    rows = {}\n    rows[identifier] = []\n\n    min = {}\n    max = {}\n    sum = {}\n    mean = {}\n    median = {}\n    std = {}\n    total = {}\n    null_counts = {}\n    non_null_counts = {}\n    \n    for column in columns:\n        if column in excludes:\n            continue\n        min[column + '-' + 'min'] = []\n        max[column + '-' + 'max'] = []\n        sum[column + '-' + 'sum'] = []\n        mean[column + '-' + 'mean'] = []\n        median[column + '-' + 'median'] = []\n        std[column + '-' + 'std'] = []\n        total[column + '-' + 'total'] = []\n        non_null_counts[column+ '-' + 'non_null'] = []\n\n    for id, group in train.groupby(identifier):\n        rows[identifier].append(id)\n        \n        for column in columns:\n            if column in excludes:\n                continue\n            min[column + '-' + 'min'].append(group[column].dropna().min())\n            max[column + '-' + 'max'].append(group[column].max())\n            sum[column + '-' + 'sum'].append(group[column].sum())\n            mean[column + '-' + 'mean'].append(group[column].mean())\n            median[column + '-' + 'median'].append(group[column].median())\n            std[column + '-' + 'std'].append(group[column].std())\n            total[column + '-' + 'total'].append(group[column].shape[0])\n            non_null_counts[column+ '-' + 'non_null'].append(pd.notnull(group[column]).sum())\n\n    records = {} \n    records['id'] = rows['id']\n    for feature in [min, max, mean, median, sum, std, total, non_null_counts]:\n        for key, values in feature.items():\n            records[key] = values\n            \n    stats_df = pd.DataFrame(records)        \n    stats_df.columns = pd.MultiIndex.from_tuples([tuple(c.split('-')) for c in stats_df.columns])\n    stats_df = stats_df.set_index('id')\n    stats_df = stats_df.stack(0).reset_index(1)\n    return stats_df",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "stats_df_der = calculate_statistics(train, derived_columns , 'id', ['id, timestamp', 'y'])\nstats_df_der.head(3)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "stats_df_fund = calculate_statistics(train, fundamental_columns , 'id', ['id, timestamp', 'y'])\nstats_df_fund.head(3)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "stats_df_tech = calculate_statistics(train, technical_columns , 'id', ['id, timestamp', 'y'])\nstats_df_tech.head(3)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def find_missing_asset_counts(column_name, column_values, data, order_dict):\n    \n    header_dict = {}\n    header_dict[column_name] = []\n    header_dict['total_present'] = []\n    header_dict['total'] = []\n    \n    for value in column_values:\n        df = data.loc[data[column_name] == value]\n        header_dict[column_name].append(value)\n        non_nulls = len(list(df.loc[df['non_null'] > 0].index))\n        header_dict['total_present'].append(non_nulls)\n        header_dict['total'].append(len(df.index))\n    \n    result = pd.DataFrame(header_dict)\n    ordered_column = next(iter(order_dict.keys()))\n    order = next(iter(order_dict.values()))\n    result = result.sort_values(by = ordered_column, ascending = order)\n    return result",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Percentage of missing assets  by for each derived column**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing_assets = find_missing_asset_counts('level_1', derived_columns, stats_df_der, {'total_present' : False})\nmissing_assets['percentage'] = (missing_assets['total_present'] / missing_assets['total']) * 100\nmissing_assets",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "missing_fun = find_missing_asset_counts('level_1', fundamental_columns, stats_df_fund, {'total_present' : False})\nmissing_fun['percentage'] = (missing_fun['total_present'] / missing_fun['total']) * 100\nmissing_fun.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Calculate the assets where derived is missing for further exploration**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "MISSING_IDS = {}\nfor column in derived_columns:\n    missing_ids_df = stats_df_der.loc[stats_df_der['level_1'] == column]\n    missing_ids_df = missing_ids_df.loc[missing_ids_df['non_null'] == 0]\n    MISSING_IDS[column] = list(missing_ids_df.index)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def find_max_col_value_by_id(dataframe, field_name, field_values, feature_values):\n    \n    data_copy = dataframe.copy(deep = True)\n    \n    headers = list(itertools.product(field_values, feature_values))\n    headers = [\":\".join(item) for item in headers]\n    headers = list(itertools.product(headers, ['high', 'low', 'highid', 'lowid']))\n    headers = [\"-\".join(item) for item in headers]\n    \n    columns_dict = {}\n    for item in headers:\n        columns_dict[item] = []\n    \n    for key in columns_dict:\n        stats_column = key.split('-')[0]\n        feature_to_find = key.split('-')[1]\n        original_column = stats_column.split(':')[0]\n        stats_feature = stats_column.split(':')[1]\n        \n        temp = data_copy.loc[data_copy[field_name] == original_column]\n        temp = temp.sort_values(by=stats_feature, ascending = False)\n        temp = temp[[stats_feature]]\n        temp = temp.reset_index()\n        \n        if feature_to_find == 'high':\n            columns_dict[key].append(temp.head(1)[stats_feature][0])\n            \n        if feature_to_find == 'highid':\n            columns_dict[key].append(temp.head(1)['id'][0])\n            \n        if feature_to_find == 'low':\n            columns_dict[key].append(temp.iloc[-1][stats_feature])\n            \n        if feature_to_find == 'lowid':\n            columns_dict[key].append(temp.iloc[-1]['id'])\n            \n    result = pd.DataFrame(columns_dict)        \n    result.columns = pd.MultiIndex.from_tuples([tuple(c.split('-')) for c in result.columns])\n    result = result.stack(0).reset_index(1)\n    \n    #data_copy = data_copy.sort_values(by=max_column, ascending = False)\n    #data_copy = data_copy[[max_column]]\n    #data_copy = data_copy.reset_index()\n    #return data_copy\n    return result",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "columns = list(stats_df_der.columns)\ncolumns.remove('level_1')\nresult = find_max_col_value_by_id(stats_df_der, 'level_1', derived_columns, columns)\nresult.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ASSETS = list(result['highid'].unique())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}