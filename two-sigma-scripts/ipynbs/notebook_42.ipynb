{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Load the data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import kagglegym\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# The \"environment\" is our interface for code competitions\nenv = kagglegym.make()\n\n# We get our initial observation by calling \"reset\"\nobservation = env.reset()\n\n# Note that the first observation we get has a \"train\" dataframe\nprint(\"Train has {} rows\".format(len(observation.train)))\n\n# The \"target\" dataframe is a template for what we need to predict:\nprint(\"Target column names: {}\".format(\", \".join(['\"{}\"'.format(col) for col in list(observation.target.columns)])))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Index the data by id and timestamp. Look at the data types and some basic info about the different columns.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_data = observation.train\ntrain_data = train_data.set_index(['id','timestamp']).sort_index()\ntrain_data",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_data.dtypes",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "f_cols=[f for f in train_data.columns if f.startswith('fundamental')]\nt_cols=[t for t in train_data.columns if t.startswith('technical')]\nID=train_data.index.levels[0]\n\nprint('The number of fundamental columns is {}.'.format(len(f_cols)))\nprint('The number of technical columns is {}.'.format(len(t_cols)))\nprint('The number of unique ids is {}.'.format(len(ID)))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Exploration of predicted variable, y",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Do some simple plots for exploration. Test for non-stationarity. Plot distributions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(5,2)\nfont = {'weight' : 'normal',\n        'size'   : 10}\nplt.rc('font', **font)\n\nrn=ID[np.random.randint(0,len(ID)-1,10)]\n\nfor i in range(0,len(rn)):\n    ax = plt.subplot(5,2,i+1)\n    ax.plot(train_data.loc[rn[i]].index, \n            train_data.y.loc[rn[i]],\n            label='ID={}'.format(rn[i]))\n    plt.legend()\n    if i in [8,9]:\n        ax.set_xlabel('Time Stamp')\n    if i in range(0,9,2):\n        ax.set_ylabel('y')    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from statsmodels.tsa.stattools import adfuller\nprint ('Results of Dickey-Fuller Test:')\ndf=pd.DataFrame([])\nfor i in rn:\n    t = adfuller(train_data.y.loc[i],\n                 autolag='AIC')\n    d={'teststat': t[0],\n       'pval': t[1],\n       'nlags': t[2],\n       'nobs': t[3],\n       '1%crit': t[4]['1%'],\n       '5%crit': t[4]['5%'],\n       '10%crit': t[4]['10%'],\n       'Nonstationary at 1% level': t[0]>t[4]['1%'],\n       'Nonstationary at 5% level': t[0]>t[4]['5%'],\n       'Nonstationary at 10% level': t[0]>t[4]['10%']}\n    df_t=pd.DataFrame(d,index=[i])\n    df=df.append(df_t)\ndf",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(5,2)\nfont = {'weight' : 'normal',\n        'size'   : 8}\nplt.rc('font', **font)\n\nfor i in range(0,len(rn)):\n    ax = plt.subplot(5,2,i+1)\n    sns.distplot(train_data.y.loc[rn[i]],\n                 norm_hist=True,kde=True,\n                 ax=ax,\n                 label='ID={}'.format(rn[i]))\n    plt.legend()\n    if i in [8,9]:\n        ax.set_xlabel('y value')\n    if i in range(0,9,2):\n        ax.set_ylabel('Prob. Density')    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Plot the power spectral densities to show contributions of various frequency components to variance of each time series. The sampling frequency is not obvious, so the units of frequency are ambiguous. From the plots, it doesn't look like there is an underlying pattern in frequency domain.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy.signal import welch\n\nfig, axs = plt.subplots(5,2)\nfont = {'weight' : 'normal',\n        'size'   : 8}\nplt.rc('font', **font)\n\nfor i in range(0,len(rn)):\n    ax = plt.subplot(5,2,i+1)\n    f,pxx=welch(train_data.y.loc[rn[i]],\n                return_onesided=True,\n                scaling='density')\n    ax.plot(f,pxx,label='ID={}'.format(rn[i]))\n    plt.legend()\n    if i in [8,9]:\n        ax.set_xlabel('Frequency')\n    if i in range(0,9,2):\n        ax.set_ylabel('Pxx')  ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Plot feature importance (from [here][1])\n\nInteresting, it looks like by reindexing the data, the feature importance has changed.\n  [1]: https://www.kaggle.com/alijs1/two-sigma-financial-modeling/quick-look-at-what-is-important-and-what-is-not",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_data = train_data.sample(frac=0.1)\ntrain_data.fillna(train_data.mean(axis=0), inplace=True)\n\n\ntrain_X = train_data.drop('y',axis=1)\ntrain_Y = train_data.y\nprint(\"Data for model: X={}, y={}\".format(train_X.shape, train_Y.shape))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = xgb.XGBRegressor()\nprint(\"Fitting...\")\nmodel.fit(train_X, train_Y)\nprint(\"Fitting done\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(figsize=(7, 30))\nxgb.plot_importance(model, ax=ax)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn import linear_model as lm\ncols_to_use = ['technical_30']\nmodels_dict = {}\nfor col in cols_to_use:\n    model = lm.LinearRegression()\n    model.fit(np.array(train_data[col]).reshape(-1,1), np.array(train.y))\n    models_dict[col] = model\n    \ncol = 'technical_30'\nmodel = models_dict[col]\nwhile True:\n    test_x = np.array(train_data.features[col].values).reshape(-1,1)\n    observation.target.y = model.predict(test_x)\n    #observation.target.fillna(0, inplace=True)\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n\n    observation, reward, done, info = env.step(target)\n    if done:\n        break",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}