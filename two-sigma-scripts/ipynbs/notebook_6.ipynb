{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Introduction to the `kagglegym` API",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Code Competitions are a new style of competition where you submit code rather than the predictions that your code creates. This allows for new types of competitions like this time-series competition hosted by Two Sigma. This notebook gives an overview of the API, `kagglegym`, which was heavily influenced by [OpenAI's Gym](https://gym.openai.com/docs) API for reinforcement learning challenges.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Data Overview\n\nAnother difference with this competition is that we're using an [HDF5 file](https://support.hdfgroup.org/HDF5/) instead of a CSV file due to the size of the data. You can still easily read it and manipulate it for exploration:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Here's an example of loading the CSV using Pandas's built-in HDF5 support:\nimport pandas as pd\n\nwith pd.HDFStore(\"../input/train.h5\", \"r\") as train:\n    # Note that the \"train\" dataframe is the only dataframe in the file\n    df = train.get(\"train\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's see how many rows are in full training set\nlen(df)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# How many timestamps are in the full training set?\nlen(df[\"timestamp\"].unique())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Important Note**: the raw training file is only available for exploration kernels. It will not be available when you make a competition submission. You should only use the raw training file for exploration purposes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## API Overview",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The \"kagglegym\" API is based on OpenAI's Gym API, a toolkit for developing and comparing reinforcement learning algorithms. Read OpenAI's Gym API [documentation](https://gym.openai.com/docs) for more details. Note that ours is named \"kagglegym\" and not \"gym\" to prevent possible conflicts with OpenAI's \"gym\" library. This section will give an overview of the concepts to get you started on this competition.\n\nThe API is exposed through a `kagglegym` library. Let's import it to get started:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import kagglegym",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now, we need to create an \"environment\". This will be our primary interface to the API. The `kagglegym` API has the concept of a default environment name for a competition, so just calling `make()` will create the appropriate one for this competition.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create environment\nenv = kagglegym.make()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To properly initialize things, we need to \"reset\" the environment. This will also give us our first \"observation\":",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get first observation\nobservation = env.reset()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Observations are the means by which our code \"observes\" the world. The very first observation has a special property called \"train\" which is a dataframe which we can use to train our model:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Look at first few rows of the train dataframe\nobservation.train.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Note that this \"train\" is about half the size of the full training dataframe. This is because we're in an exploratory mode where we simulate the full environment by reserving the first half of timestamps for training and the second half for simulating the public leaderboard.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get length of the train dataframe\nlen(observation.train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get number of unique timestamps in train\nlen(observation.train[\"timestamp\"].unique())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Note that this is half of all timestamps:\nlen(df[\"timestamp\"].unique())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Here's proof that it's the first half:\nunique_times = list(observation.train[\"timestamp\"].unique())\n(min(unique_times), max(unique_times))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Each observation also has a \"features\" dataframe which contains features for the timestamp you'll be asked to predict in the next \"step.\" Note that these features are for timestamp 906 which is just passed the last training timestamp. Also, note that the \"features\" dataframe does *not* have the target \"y\" column:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Look at the first few rows of the features dataframe\nobservation.features.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The final part of observation is the \"target\" dataframe which is what we're asking you to fill in. It includes the \"id\"s for the timestamp next step.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Look at the first few rows of the target dataframe\nobservation.target.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This target is a valid submission for the step. The OpenAI Gym calls each step an \"action\". Each step of the environment returns four things: \"observation\", \"reward\", \"done\", and \"info\".",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Each step is an \"action\"\naction = observation.target\n\n# Each \"step\" of the environment returns four things:\nobservation, reward, done, info = env.step(action)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The \"done\" variable tells us if we're done. In this case, we still have plenty of timestamps to go, so it returns \"False\".",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print done\ndone",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The \"info\" variable is just a dictionary used for debugging. In this particular environment, we only make use of it at the end (when \"done\" is True).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print info\ninfo",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We see that \"observation\" has the same properties as the one we get in \"reset\". However, notice that it's for the next \"timestamp\":",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Look at the first few rows of the observation dataframe for the next timestamp\nobservation.features.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Note that this timestamp has more id's/rows\nlen(observation.features)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Perhaps most interesting is the \"reward\" variable. This tells you how well you're doing. The goal in reinforcement contexts is that you want to maximize the reward. In this competition, we're using the R value that ranges from -1 to 1 (higher is better). Note that we submitted all 0's, so we got a score that's below 0. If we had correctly predicted the true mean value, we would have gotten all zeros. If we had made extreme predictions (e.g. all `-1000`'s) then our score would have been capped to -1.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print reward\nreward",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Since we're in exploratory mode, we have access to the ground truth (obviously not available in submit mode):",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nperfect_action = df[df[\"timestamp\"] == observation.features[\"timestamp\"][0]][[\"id\", \"y\"]].reset_index(drop=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Look at the first few rows of perfect action\nperfect_action.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's see what happens when we submit a \"perfect\" action:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Submit a perfect action\nobservation, reward, done, info = env.step(perfect_action)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As expected, we get the maximum reward of 1 by submitting the perfect value:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print reward\nreward",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Making a complete submission",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We've covered all of the basic components of the `kagglegym` API. You now know how to create an environment for the competition, get observations, examine features, and submit target values for a reward. But, we're still not done as there are more observations/timestamps left.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print done ... still more timestamps remaining\ndone",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now that we've gotten the basics out of the way, we can create a basic loop until we're \"done\". That is, we'll make a prediction for the remaining timestamp in the data:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "while True:\n    target = observation.target\n    timestamp = observation.features[\"timestamp\"][0]\n    if timestamp % 100 == 0:\n        print(\"Timestamp #{}\".format(timestamp))\n\n    observation, reward, done, info = env.step(target)\n    if done:        \n        break",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now we can confirm that we're done:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print done\ndone",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And since we're \"done\", we can take a look at at \"info\", our dictionary used for debugging. Recall that in this environment, we only make use of it when \"done\" is True.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print info\ninfo",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Our score is better than 0 because we had that one submission that was perfect.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Print \"public score\" from info\ninfo[\"public_score\"]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This concludes our overview of the `kagglegym` API. We encourage you to ask questions in the competition forums or share public kernels for feedback on your approach. Good luck!",
      "metadata": {}
    }
  ]
}