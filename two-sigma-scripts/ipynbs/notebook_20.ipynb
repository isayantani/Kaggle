{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Cluster Visualization - Assets based on Structural NAN values**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nimport matplotlib.pyplot as plt\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#load the data. It comes in train.h5 so we need to us HDFStore\ndf = pd.HDFStore(\"../input/train.h5\", \"r\").get(\"train\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# I make vectors that hold the ratio of NAN values for a given id in a given column\nunique_ids = pd.unique(df.id)\nlen(unique_ids)\nNaN_vectors = np.zeros(shape=(len(unique_ids), df.shape[1]))\n\nfor i, i_id in enumerate(unique_ids):\n    data_sub = df[df.id ==i_id]\n    NaN_vectors[i,:] = np.sum(data_sub.isnull(),axis=0) /float(data_sub.shape[0])\n    \nNaN_vectors",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# get all the NaN vectors in which every collumn for that ID is NaN. What we are looking for \n#is collumns in which the features fundamentally do not exist. \nbin_NaN = 1*(NaN_vectors==1)\nprint(\"Still has the shape of {} by {}\".format(bin_NaN.shape[0],bin_NaN.shape[1]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# we now have a vector of things that are either 1 where nothing exists in teh column or zero something\n#exists Now we take a covariance over these bins to see which ones move togehter. we are looking only based on columns\nbin_cov=np.corrcoef(bin_NaN.T)\nbin_cov.shape[1]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# plot bin_cov\nplt.matshow(bin_cov)\n\n#if you think abou what this shows it is show the probability that when an entire column is missing what\n# is the probability that another column will be completely missing. ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# In this graph i make the matrix sparse by considering only things that have perfect correlation. This\n# gives us insight into the relationship of the pairs.\nplt.matshow(bin_cov == 1) ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# What we are doing here is looking at all the column pairs that when they are missing  they are always\n# missing together. ie they have a corralation of 1. We also get the count. it stands to reason that\n# if this happens in only one or two id's out of 1400 then perhaps it is a statistical anomoly or could be \n# reflective of a non structural issue. This is actually very enlightening and we see there are 60\n# some odd pairs that satisfy this criteria. More importantly is that it happens for lots of tickers.\n# Maybe we have soemthing here.\nbin_NaN\nedges = []\ncount =np.dot(bin_NaN.T,bin_NaN)\nfor i in range(bin_cov.shape[0]):\n    for j in range(bin_cov.shape[0]-i):\n        if i!=i+j and bin_cov[i,i+j]==1:\n            edges.append([i,i+j,count[i,i+j]])\nprint(edges)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#lets see how many unique counts there are. it looks like a few of these counts happen multiple times.\n# this is interesting and could imply some structural issue.\nucount = [i[2] for i in edges]\nprint(np.unique(ucount))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print('rows: {}'.format(bin_NaN.shape[0]))\nprint('cols: {}'.format(len(edges)))\n\n# the idea here is that we create a feature vector. We look at all the ids which have all their data\n# missing in a certain collumn. above we found that if all the data is missing in a certain collumn it\n# would be missing in another collumn as well. so we look at all these pairs (shown as edges) and we \n# then create a matrix of id x edges. We then put a 0 or a 1 in the collumn to indicate that the pair of \n# data is missing or not. This serves as a feature. I will then go on to cluster over these features. \n\nnan_features = np.zeros((bin_NaN.shape[0],len(edges)))\nfor i in range(bin_NaN.shape[0]):\n    for j, edge in enumerate(edges):\n        nan_features[i,j] = 1*(bin_NaN[i,edge[0]] & bin_NaN[i,edge[1]])\n\nprint('this is just a check that indexing is correct: {}'.format(np.sum(nan_features,axis=1).shape[0]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# we take a look at the silouette score as we increase the number of clusters to understand the optimal\n#number of clusters. We see here that it continues to increase which we would expect. I chose to cut it\n# off around 12\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n#Range for k\nkmin = 2\nkmax = 25\nsil_scores = []\n\n#Compute silouhette scoeres\nfor k in range(kmin,kmax):\n    km = KMeans(n_clusters=k, n_init=20).fit(nan_features)\n    sil_scores.append(silhouette_score(nan_features, km.labels_))\n\n#Plot\nplt.plot(range(kmin,kmax), sil_scores)\nplt.title('KMeans Results')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# DBSCAN The only thing that is important from here is the labels. \n# the way that DB scan works is that you give it eps and min_samples and it\n# finds core groups. eps is the distance cut off and min is how many elements\n# at minimum you need to define a cluster\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\n\nX = nan_features\n\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#COLOR MAPPING - we run a kmeans cluster but we need to decide how many\n#clusters we want to use. This is another way for us to cluster. Here we use \nk=12\nkm = KMeans(n_clusters=k, n_init=20).fit(nan_features)\ncolors=km.labels_",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#now we try to visualize the data of these features.\n# WOW LOOK AT THESE RESULTS. THAT IS BEAUTIFUL!!\n\nfrom sklearn.manifold import TSNE\nfrom time import time\n\nn_iter = 5000\n\nfor i in [2, 5, 30, 50, 100]:\n    t0 = time()\n    model = TSNE(n_components=2, n_iter = n_iter,random_state=0, perplexity =i)\n    np.set_printoptions(suppress=True)\n    Y = model.fit_transform(nan_features)\n    t1 =time()\n\n    print( \"t-SNE: %.2g sec\" % (t1 -t0))\n    plt.scatter(Y[:, 0], Y[:, 1], c= colors)\n    plt.title('t-SNE with perplexity = {}'.format(i))\n    plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Now i do the same in 3d to try to better understand these clusters\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nn_iter = 5000\n\nfor i in [2, 5, 30, 50, 100]:\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    t0 = time()\n    model = TSNE(n_components=3, random_state=0, perplexity=i, n_iter=n_iter)\n    np.set_printoptions(suppress=True)\n\n    Y = model.fit_transform(nan_features)\n    t1 =time()\n\n    print( \"t-SNE: %.2g sec\" % (t1 -t0))\n\n    ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2],c=db.labels_,\n               cmap=plt.cm.Paired)\n    ax.set_title(\"3D T-SNE - Perplexity = {}\".format(i))\n    ax.set_xlabel(\"1st dim\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd dim\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd dim\")\n    ax.w_zaxis.set_ticklabels([])\n    plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# I will use PCA now to plot\nfrom sklearn import decomposition\n\n# I chose this number pretty much at random, you can change it. using 22 features to describe\n# something with 110 x variables still seems high.\nn_eigens = 22\n# Creating PCA object\npca = decomposition.PCA(n_components=n_eigens, svd_solver ='randomized', whiten=True)\nX_pca =pca.fit_transform(nan_features)\nX_pca",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**2D -PCA Plot**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This is a 2D pca plot... mehhhh\nplt.scatter(X_pca[:,0],X_pca[:,1],c=colors)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = decomposition.PCA(n_components=3).fit_transform(nan_features)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2],c=colors,\n           cmap=plt.cm.Paired)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# here we plot a graph that looks at how many PCA components explain the variation\nn_eigens=10\nX_reduced = decomposition.PCA(n_components=n_eigens).fit(nan_features)\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=(8, 5));\n    plt.title('Explained Variance Ratio over Component');\n    plt.plot(X_reduced.explained_variance_ratio_);",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "with plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=(8, 5));\n    plt.title('Cumulative Explained Variance over EigenFace');\n    plt.plot(X_reduced.explained_variance_ratio_.cumsum());",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print('PCA captures {:.2f} percent of the variance in the dataset'.format(X_reduced.explained_variance_ratio_.sum() * 100))\nprint('PCA components have dimensions {} by {}'.format(*X_reduced.components_.shape))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}