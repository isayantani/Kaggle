{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# RNN Model Production and Exploration Notebook\n\nThis notebook is an attempt to build some DRNN structure to predict market movements on TWOSIGMA Financial Modeling dataset.\n### This is a work in progress!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import modules\nimport gc\ngc.collect()\nimport kagglegym\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, LSTM\nfrom keras.regularizers import l2\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn import preprocessing as pp\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Start environment\nenv = kagglegym.make()\nobservation = env.reset()\ntrain = observation.train",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Lets print our dataset head to see how it looks like:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "observation.train.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Lets preprocess the data filling the NaNs values with the median of that columns values, as seen so many times on the community work. Also, lets clip our features to use between the min and max values of the target variable in an attempt to deal with the outliers, and scale the data in order to help the neuronnet to learn.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Data preprocessing\n\n# https://www.kaggle.com/bguberfain/two-sigma-financial-modeling/univariate-model-with-clip/run/482189/code\n# Clipped target value range to use\nlow_y_cut = -0.086093\nhigh_y_cut = 0.093497\n\ny_is_above_cut = (train.y > high_y_cut)\ny_is_below_cut = (train.y < low_y_cut)\ny_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\n\n# Select the features to use\nexcl = ['id', 'sample', 'y', 'timestamp']\n#feature_vars = [c for c in train.columns if c not in excl]\nfeatures_to_use =  ['technical_20', 'technical_30', 'fundamental_11',\n                    'fundamental_37', 'technical_35', 'technical_36', 'fundamental_36']\ntarget_var = ['y']\n\nfeatures = train.loc[y_is_within_cut, features_to_use]\nX_train = features.values\n\ntargets = train.loc[y_is_within_cut, target_var]\ny_train = targets.values\n\nim = pp.Imputer(strategy='median')\nX_train = im.fit_transform(X_train)\nX_scaler = pp.StandardScaler()\nX_train = X_scaler.fit_transform(X_train)\ny_scaler = pp.StandardScaler()\n#y_train = y_scaler.fit_transform(y_train.reshape([-1,1]))\n\nX_train = pd.DataFrame(X_train, columns=features_to_use)\ny_train = pd.DataFrame(y_train, columns=target_var)\npreprocess_dict = {'fillna': im, 'X_scaler': X_scaler, 'y_scaler': y_scaler}\n\ndel y_is_above_cut, y_is_below_cut, excl, targets, features",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We have selected the most important features to use as found in some of the community work, specifically in [this notebook](https://www.kaggle.com/fernandocanteruccio/two-sigma-financial-modeling/xgboost-feature-importance-analysis). Lets take a peek in our dataset head again.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_train.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Right! Now we have scaled values and without NaN values. Better this way!\nNow we can start to build our models. This time we gonna try some deep neural network arquitetures and see how it performs. Let's get started!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Model Definition\ndef dnn(shape,timesteps,l2_coef,drop_coef):\n    model = Sequential()\n    model.add(LSTM(shape[1], init='glorot_normal', W_regularizer=l2(l2_coef),\n                   return_sequences=True, input_shape=(timesteps, shape[0])))\n    model.add(LSTM(shape[2], init='glorot_normal', W_regularizer=l2(l2_coef),\n                   return_sequences=False))\n    model.add(Dense(shape[3], init='glorot_normal', activation='elu', W_regularizer=l2(l2_coef)))\n    model.add(Dense(shape[4], init='glorot_normal', activation='linear', W_regularizer=l2(l2_coef)))\n\n    optm = RMSprop(lr=0.007, rho=0.9, epsilon=1e-08, decay=0.0)\n    model.compile(loss='mean_squared_error',\n                  optimizer=optm,\n                  metrics=['mean_squared_error'],\n                  verbose=2)\n    return model",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Starting with just one layer of LSTM neurons and two fully-connected layers on the output.\nHere we apply L2 regularization in an attempt to deal with overfiting. We also use some dropout and early-stop  techniques. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Training Routine\ntimesteps = 8\nbatch_size = 2**14\ntraining_epochs = 15\n\nprint(\"Padding Sequences\")\nt0 = time()\nX_train_ts = np.array([pad_sequences(X_train[col].values.reshape([-1,1]), maxlen=timesteps, dtype='float32') \n              for col in X_train]).transpose((1, 2, 0))\nprint(\"Done! Padding time:\", time() - t0)\nprint(\"X_train shape:\",X_train_ts.shape)\nprint(\"Training model\")\nt0 = time()\nmodel1 = dnn(shape=[X_train_ts.shape[2],8,8,128,1],timesteps=timesteps,l2_coef=1e-9,drop_coef=1.0)\nmodel1.fit(X_train_ts, y_train.values,\n          nb_epoch=training_epochs,\n          batch_size=batch_size,\n          callbacks=[ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=1e-7,\n                                       epsilon=0.0001, verbose=1),\n                     EarlyStopping(monitor='loss', min_delta=0.00001, patience=12, verbose=1, mode='auto')],\n          shuffle=False,\n          verbose=2\n          );\n\nprint(\"Done! Training time:\", time() - t0)\ndel X_train, t0",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Analysing Training Results",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Predicting target on training dataset\")\nt0 = time()\nm1_preds = model1.predict(X_train_ts, batch_size=batch_size, verbose=0)\nscore = r2_score(y_train, m1_preds)\nprint(\"Done! Prediction time:\",time() - t0)\nprint(\"R2 score for train dataset\",score)\n\ndel X_train_ts, y_train, score, t0",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "market_df = observation.train[['timestamp', 'y']].groupby('timestamp').agg([np.mean, np.std]).reset_index()\ny_mean = np.array(market_df['y']['mean'])\ny_std = np.array(market_df['y']['std'])\nt = market_df['timestamp']\n\ncum_ret = np.log(1+y_mean).cumsum()\npred_ret = pd.DataFrame(np.vstack((observation.train.timestamp.loc[y_is_within_cut], m1_preds[:,0])).T,\n                        columns=['timestamp','y']).groupby('timestamp').agg([np.mean, np.std]).reset_index()\npred_std = np.array(pred_ret['y']['std'])\ncum_pred = np.log(1+pred_ret['y']['mean']).cumsum()\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set_xlabel(\"Timestamp\");\nax.set_title(\"Cumulative target signal and predictions over time\");\nsns.tsplot(cum_ret,t,ax=ax,color='b');\nsns.tsplot(y_std,t,ax=ax,color='g');\nsns.tsplot(cum_pred,t,ax=ax,color='r');\nsns.tsplot(pred_std,t,ax=ax,color='black');\nax.set_ylabel('Target / Prediction');\n\nfig, ax = plt.subplots(figsize=(12,7))\nax.set_title(\"Target Variable Distribution. (True vs Prediction)\");\nplt.ylim([0, 50000])\nsns.distplot(observation.train.y ,ax=ax, color='b', kde=False, bins=100);\nsns.distplot(m1_preds ,ax=ax, color='r', bins=100);\nax.set_ylabel('Target / Prediction');\n\nweights = model1.get_weights()\nfig, ax = plt.subplots(figsize=(12,7))\nax.set_title(\"Weights Variable Distribution.\");\n#plt.ylim([0, 2])\nsns.distplot(weights[0].flatten() ,ax=ax, color='b');\nsns.distplot(weights[1].flatten() ,ax=ax, color='r');\nsns.distplot(weights[2].flatten() ,ax=ax, color='g');\nsns.distplot(weights[3].flatten() ,ax=ax, color='black');\nax.set_ylabel('Occurences');\n#print(weights[5])\ndel market_df, y_mean, y_std, t, cum_ret, pred_ret, cum_pred",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Using the model for predictions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Predict-step-predict routine\ndef gen_preds(model, preprocess_dict, features_to_use, print_info=True):\n    env = kagglegym.make()\n    # We get our initial observation by calling \"reset\"\n    observation = env.reset()\n\n    im = preprocess_dict['fillna']\n    X_scaler = preprocess_dict['X_scaler']\n    y_scaler = preprocess_dict['y_scaler']\n    \n    reward = 0.0\n    reward_log = []\n    timestamp_log = []\n    pred_log= []\n    pos_count = 0\n    neg_count = 0\n\n    total_pos = []\n    total_neg = []\n\n    print(\"Predicting\")\n    t0= time()\n    while True:\n    #    observation.features.fillna(mean_values, inplace=True)\n\n        # Predict with model\n        features_dnn = im.transform(observation.features.loc[:,features_to_use].values)\n        features_dnn = pd.DataFrame(X_scaler.transform(features_dnn),columns=features_to_use)\n        features_dnn_ts = np.array([pad_sequences(features_dnn[col].values.reshape([-1,1]), maxlen=timesteps,\n                                                  dtype='float32') for col in features_to_use]).transpose((1, 2, 0))\n\n        y_dnn = model.predict(features_dnn_ts,batch_size=features_dnn.shape[0],\n                              verbose=0).clip(low_y_cut, high_y_cut)\n\n        # Fill target df with predictions \n#        observation.target.y = y_scaler.inverse_transform(y_dnn)\n        observation.target.y = y_dnn\n        observation.target.fillna(0, inplace=True)\n        target = observation.target\n        timestamp = observation.features[\"timestamp\"][0]\n        \n        observation, reward, done, info = env.step(target)\n\n        timestamp_log.append(timestamp)\n        reward_log.append(reward)\n        pred_log.append(y_dnn)\n\n        if (reward < 0):\n            neg_count += 1\n        else:\n            pos_count += 1\n\n        total_pos.append(pos_count)\n        total_neg.append(neg_count)\n        \n        if timestamp % 100 == 0:\n            if print_info:\n                print(\"Timestamp #{}\".format(timestamp))\n                print(\"Step reward:\", reward)\n                print(\"Mean reward:\", np.mean(reward_log[-timestamp:]))\n                print(\"Positive rewards count: {0}, Negative rewards count: {1}\".format(pos_count, neg_count))\n                print(\"Positive reward %:\", pos_count / (pos_count + neg_count) * 100)\n\n            pos_count = 0\n            neg_count = 0\n\n        if done:\n            break\n    print(\"Done: %.1fs\" % (time() - t0))\n    print(\"Total reward sum:\", np.sum(reward_log))\n    print(\"Final reward mean:\", np.mean(reward_log))\n    print(\"Total positive rewards count: {0}, Total negative rewards count: {1}\".format(np.sum(total_pos),\n                                                                                        np.sum(total_neg)))\n    print(\"Final positive reward %:\", np.sum(total_pos) / (np.sum(total_pos) + np.sum(total_neg)) * 100)\n    print(info)\n    return np.array(pred_log), np.array(reward_log), np.array(timestamp_log)\n\npred_log, reward_log, timestamp_log = gen_preds(model1, preprocess_dict, features_to_use)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}