{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Simple Notebook for exploring the data ...**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nfrom scipy import signal\nimport matplotlib.pyplot as plt\n\nimport kagglegym",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This part is going to be for explorind the dataset ...\n# so we want the entire dataset ..\nwith pd.HDFStore(\"../input/train.h5\", \"r\") as train:\n    df = train.get(\"train\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list(set([c.split('_')[0] for c in df.columns]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "So there are three types of main cells. \n\n - `timestamp`: current timestamp\n - `y`: This is what we want to predict\n - [`fundamental`, `derived`, `technical`]: these are our predictors",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Finding distributions of the result\n\ndf1 = df[['timestamp', 'y']].groupby('timestamp').agg([np.mean, np.std, len]).reset_index()\ndf1.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n     = df1['timestamp']\nyMean = np.array(df1['y']['mean'])\nyStd  = np.array(df1['y']['std'])\n\nplt.figure()\nplt.plot(n, yMean, '.')\nplt.xlabel('n')\nplt.ylabel('$y_{mean}$[n]')\n\nplt.figure()\nplt.plot(n, yStd, '.')\nplt.xlabel('n')\nplt.ylabel('$y_{std}$[n]')\n\nplt.figure()\nplt.plot(np.array(df1['y']['len']), yStd, '.')\nplt.xlabel('len')\nplt.ylabel('$y_{std}$[n]')\n\nplt.figure()\nplt.plot(np.array(df1['y']['len']), yMean, '.')\nplt.xlabel('len')\nplt.ylabel('$y_{mean}$[n]')\n\nplt.figure()\nplt.plot(np.diff(yMean), '.')\nplt.xlabel('n')\nplt.ylabel('$y_{mean}$[n] $-$ $y_{mean}$[n-1]')\n\nplt.figure()\nplt.plot( yMean[:-1] , yMean[1:] , '.')\nplt.xlabel('$y_{mean}$[n]')\nplt.ylabel('$y_{mean}$[n-1]')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Wow, there seems to be absolutely no correlations ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Calculate running averages ...\n#yMeanRA = np.cumsum( yMean )/np.linspace( 1, len(yMean), len(yMean) )\n#yStdRA  = np.cumsum( yStd )/np.linspace( 1, len(yMean), len(yMean) )\n\nyMeanCumsum = np.cumsum( yMean )\nf, t, Syy   =  signal.spectrogram(yMeanCumsum)\n\nslope1 = np.mean(yMean)\nslope2 = np.mean(df['y'])\nslope3 = np.mean(df['y'][df.timestamp < 906])\n\nplt.figure()\nplt.plot(n, yMeanCumsum, '.')\nplt.plot(n, slope1*n, color='black', lw=2 ) \nplt.plot(n, slope2*n, color='orange', lw=2 ) \nplt.plot(n, slope2*n, color='indianred', lw=2 ) \nplt.xlabel('n')\nplt.ylabel('$y_{mean}$[n] - Cumsum')\n\nplt.figure()\nplt.pcolormesh(t, f, np.log10(Syy))\nplt.ylabel('Frequency')\nplt.xlabel('Time')\nplt.colorbar()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The cumulative sum seems to be increasing linearly. Is there something there??? I need to find a linear trend and see of there is something that can be obtained form there. The linear ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Remember that the training contains timestamps upto 905\nnp.mean(yMean), np.mean(df['y']), np.mean(df['y'][df.timestamp < 906])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This means that the mean of y is greater than zero. This is great. This tells us that the slope overall should be positive. Now, lets find out if we will be able to find significant deviations from the mean. The way we want to check is to see of the standard deviations for a day is a meaningful measure of predicting deviations from the mean ...\n\nThe following calculations are not realistic because we have a *look-ahead bias*. But right now we don't care.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dev = yMeanCumsum - np.mean(yMean)*n\n\nplt.figure()\nplt.plot(n, dev, '.')\nplt.axhline(color='black', ls='--')\nplt.xlabel('n')\nplt.ylabel('$y_{mean}$[n] - without the slope')\n\nplt.figure()\nplt.plot(dev[1:], dev[:-1], '.')\nplt.xlabel('$y_{std}$ [n]')\nplt.ylabel('$y_{std}$ [n-1]')\n\nplt.figure()\nplt.plot(np.abs(dev), yStd, '.')\nplt.xlabel('deviations from the mean')\nplt.ylabel('$y_{std}$')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def getScore(slope):\n    rewards = []\n    print(slope)\n    env = kagglegym.make()\n    observation = env.reset()\n\n    while True:\n        target    = observation.target\n        timestamp = observation.features[\"timestamp\"][0]\n        target['y'] = slope\n\n        observation, reward, done, info = env.step(target)\n        rewards.append(reward)\n        if done: break\n            \n    return info['public_score'], rewards",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "slope = 0.0002182\nx1 = np.linspace(slope - 0.0001, slope+0.0001 , 4)\ny1 = [ getScore(m) for m in x1 ]\ny1, rewards = zip(*y1)\n\nplt.plot(x1, y1, 's-', color='green', mfc='green', mec='black')\nplt.figure()\nfor r in rewards:\n    plt.plot(r)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I have to figure out what this rewards this is ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "list(map(np.mean, rewards))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "columns = [c for c in df.columns if 'fundamental' in c]\ncolumns1 = columns + ['timestamp']\ndf2 = df[columns1].groupby('timestamp').agg([np.mean]).reset_index()\n\ni = 0; N=5\nwhile True:\n    \n    if i >= len(columns): break\n    for j in range(N):    \n        if i >= len(columns): break\n        \n        if j == 0:\n            plt.figure(figsize=(8,8.0/N))\n        \n        plt.axes([j*1.0/N,0,1.0/N,1])\n        plt.plot(df2['timestamp'], df2[ columns[i] ])\n        plt.xticks([]); plt.yticks([])\n        i += 1 ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Looks like there are a lot of features which correspond very closely to each other. Also, some of them are correlated to the cumulative sum. We want to check if there is some sort of correlation between the different values of fundamental, and y",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import seaborn as sns\ncorrColumns = [c for c in df2.columns if 'timestamp'not in c]\nfundamentals = np.array([df2[c] for c in corrColumns])\nCorr = np.corrcoef( dev, fundamentals )\nsns.clustermap( pd.DataFrame(np.abs(Corr), columns=['---y--']+[c[0].split('_')[1] for c in corrColumns]) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A whole lot of stuff is correlated to the deviation form the mean. This is what we are trying to predict. ",
      "metadata": {}
    }
  ]
}