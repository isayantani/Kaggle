{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Hello Kaggle,\n\nHere's a few of my observations building upon the previous work on reverse engineering y values for the previous timestep. I do wonder if 2sigma wants us to look at this. If they wanted us to use y(t-1), they could have just provided it. Unless all of this is just a recruitment puzzle. But I digress.\n\nChenjx1005 really laid the groundwork, lets take a look at that model first.\n\nhttps://www.kaggle.com/chenjx1005/two-sigma-financial-modeling/physical-meanings-of-technical-20-30/discussion",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# setup\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(\"Loading file data.\") ; \nwith pd.HDFStore('../input/train.h5', \"r\") as train: train = train.get(\"train\") \nmin_y, max_y = min(train.y), max(train.y)\ntrain.sort_values(by=['id', 'timestamp'], inplace=True)\ntrain['y1'] = train.groupby('id')['y'].shift(1).fillna(0) # setup y(t-1) values",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# my interpretation of the model from:\n# https://www.kaggle.com/chenjx1005/two-sigma-financial-modeling/physical-meanings-of-technical-20-30/discussion\n# using formula from\n# https://www.kaggle.com/c/two-sigma-financial-modeling/discussion/29142\n\nalpha1 = 0.92\nalpha0 = 0.07\ntrain['f0'] = train['technical_20'] - train['technical_30'] + train['technical_13'] \ntrain['f1'] = train.groupby('id')['f0'].shift(1).values\ntrain['f2'] = ( train['f0'] - train['f1'] * alpha1 ) / alpha0\ntrain['f2'] = train['f2'].clip(min_y,max_y).fillna(0)\n\ntrain['f2d'] = train['y1'] - train['f2'] # check difference between true y1\n\nprint('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\nplt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\n\nn0s = 10000\nplt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) # use transparency to see dense regions\nplt.grid() ; plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Rbauld's notebook proposed using ewma's of a few other features, let's evaluate.\n# It seems like t13 should be added as well.\n# https://www.kaggle.com/rbauld/two-sigma-financial-modeling/rebuilding-y-t-1/discussion\n\ntrain['y_shifted'] = train['y1']\ndef ewm_mean(x,span_in):\n    return(x.ewm(span=span_in).mean())\ntrain['EWM_26_mean_s']  = train.groupby('id')['y_shifted'].apply(lambda x: ewm_mean(x,span_in=26))\n\n# ewm_features = ['technical_30','technical_20','technical_21','technical_19','technical_17','technical_11','technical_2']\n# t13 added to ewm features\newm_features = ['technical_30','technical_20','technical_21','technical_19','technical_17','technical_11','technical_2','technical_13']\nmean_values = train[ewm_features].mean(axis=0)\ntrain[ewm_features] = train[ewm_features].fillna(mean_values)\n\n# n0 = int(len(train)/4) # really slow\nn0 = 100000\nimport sklearn as sk\nfrom sklearn import ensemble\nmy_model = sk.ensemble.GradientBoostingRegressor(loss='ls', max_depth=5, learning_rate=0.05)\nmy_model.fit(X=train.loc[:n0,ewm_features],y=train.loc[:n0,'EWM_26_mean_s'])\ntrain['EWM_26s_pred'] = my_model.predict(X=train[ewm_features]) \n\n# Inverse transform\ndef ewm_reverse(data,span=26):\n    alpha = 2/(span+1)\n    return (data-(1-alpha)*data.shift(1).fillna(0))/alpha\ntrain['yEWM_26'] = train.groupby('id')['EWM_26s_pred'].apply(lambda x: ewm_reverse(x, span=26))\n\ntrain['f2'] = train['yEWM_26'].clip(min_y,max_y).fillna(0)\ntrain['f2d'] = train['y1'] - train['f2']\n\nprint('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\nplt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\nn0s = 10000\nplt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) # use transparency to see dense regions\nplt.grid() ; plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# It seems more elegant to only have a single alpha value, and that they would be related\n# (As noted by Ricardus in the thread)\n# this also allows us to scale the data easily\n# lets look at the cases without t13, with t13 added and with t13 subtracted.\n\nalpha0 = 0.9327 # identified by manual newtonian maximization\n\ntitle0 = 't20-t30'\ntrain['f0'] = train['technical_20'] - train['technical_30'] # + train['technical_13'] \ntrain['f1'] = train.groupby('id')['f0'].shift(1).values\ntrain['f2'] = train['f0'] - train['f1'] * alpha0\ntrain['f2'] = train['f2'] / (1-alpha0) # scale\ntrain['f2'] = train['f2'].clip(min_y,max_y).fillna(0)\ntrain['f2d'] = train['y1'] - train['f2'] # check difference between true y1\nprint(title0) ; print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\nplt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\nn0s = 10000 ; plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) ; plt.grid() ; plt.show()\n\ntitle0 = 't20-t30-t13'\ntrain['f0'] = train['technical_20'] - train['technical_30'] - train['technical_13'] \ntrain['f1'] = train.groupby('id')['f0'].shift(1).values\ntrain['f2'] = ( ( train['f0'] - train['f1'] * alpha0 ) / (1-alpha0) ).clip(min_y,max_y).fillna(0)\ntrain['f2d'] = train['y1'] - train['f2'] # check difference between true y1\nprint(title0) ; print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\nplt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\nn0s = 10000 ; plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) ; plt.grid() ; plt.show()\n\ntitle0 = 't20-t30+t13'\ntrain['f0'] = train['technical_20'] - train['technical_30'] + train['technical_13'] \ntrain['f1'] = train.groupby('id')['f0'].shift(1).values\ntrain['f2'] = ( ( train['f0'] - train['f1'] * alpha0 ) / (1-alpha0) ).clip(min_y,max_y).fillna(0)\ntrain['f2d'] = train['y1'] - train['f2'] # check difference between true y1\nprint(title0) ; print('Number points within 0.0005 of true y: {}'.format(np.sum( abs(train['f2d']) < 0.0005 )) )\nplt.hist(train['f2d'],bins=200) ; plt.grid() ; plt.show()\nn0s = 10000 ; plt.scatter( train['f2'][:n0s], train['y1'][:n0s], alpha=0.1 ) ; plt.grid() ; plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "So we can see the single alpha model works better.\n\nWe see an improvement in accuracy both when we add t13 and when we subtract t13. I couldn't figure out when to add or subtract.\n\nIn the t20-t30 plot you can see a vertical line of samples at the axis that goes away in the +t13 and -t13 plots. You can also see all of the samples in the -t13 plot shooting off on the opposite slope. Those seem to be the samples where t13 should be added. It's also present on the +t13 plot.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Errors from t20t30t13 are frequently coincident, but there are other error components\n\nplt.figure(figsize=(12,5))\nn0s, n1s = 0, 200\n\nids0 = train.id.unique()\ntimestamps0 = train.timestamp.unique()\nfor id in ids0[:11]:\n    rows0 = train.id.isin([id]) & train.timestamp.isin(timestamps0[n0s:n1s])\n    plt.plot( train.loc[rows0,'timestamp'], train.loc[rows0,'f2d'] )\nplt.grid()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}