{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "As shown before, some features per id can be clustered intro groups that perform the same kind of time evolution.  Unfortunately a unique feature does not perform the same dynamics for different ids. But maybe there are groups of ids that have identical or similar features and perhaps this groups can give some insights into the \"global\" dynamics. \n\n## Assumptions: ##\n\n - NaN values make sense. An id that has always NaNs for a specific feature has no relationship with it.  \n - Id's live in different time-zones (and have different lifetimes)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport kagglegym\nimport matplotlib.pyplot as plt\n\nwith pd.HDFStore(\"../input/train.h5\", \"r\") as train:\n    # Note that the \"train\" dataframe is the only dataframe in the file\n    df = train.get(\"train\")\n\n# Create an environment\nenv = kagglegym.make()\n\n# Get first observation\nobservation = env.reset()\n\n# Get the train dataframe\ntrain = observation.train",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Lifetimes** \n\nFirst, I will pick up the idea of Chase, to [look at id lifetimes][1]. There are a lot of ids having a large number of timestamps and maybe they could give more insights into the time evolution and dynamics of features. Perhaps one could also find id-groups of same lifetimes with similar behavior. \n\n  [1]: https://www.kaggle.com/chaseos/two-sigma-financial-modeling/understanding-id-and-timestamp",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "lifetimes = df.groupby('id').apply(len)\nlifetimes = lifetimes.sort_values(ascending=False)\nlifetimes = lifetimes.reset_index()\nlifetimes.columns = [\"id\", \"duration\"]\nlifetimes.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's collect all id's that have a lifetime of 1813.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "long_lifetime_ids = lifetimes[lifetimes[\"duration\"] == 1813]\nlong_lifetime_ids.info()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Select ids of same nan-structure**\n\nThere are 527 ids with a lifetime of 1813 timestamps. As I want to study the feature dynamics of id's that behave the same way, I need to find all those id's that share the same features (and nan-structures). Let's do this by a simple approach: Select one example id of the dataframe \"long_lifetimes_ids\" and find all ids in that frame that match its feature-presence (nan-structure). \n\nI will be careful with binary transformations because a nan-value may not be present all over the time of my selected id and perhaps 0.0 is the first value that occurs in the situation where \"nan\" changes to a value.  Instead, I will do the following:\n\n - Kick out features that have a permanent nan-structure over all 1813 timestamps\n - Keep features with partial present nan-structures, but collect their labels for safety ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "long_lifetime_ids.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def find_nan_structure(instrument, data):\n    data_id = data.loc[data[\"id\"]==instrument,:]\n    no_nan_features = []\n    partial_nan_features = []\n    total_nan_features = [] \n    for col in data_id.columns:\n        if col not in [\"id\", \"timestamp\", \"y\"]:\n            nr_nans = pd.isnull(data_id[col]).sum()\n            if (nr_nans == 0):\n                no_nan_features.append(col)\n            elif (nr_nans == len(data_id[col])):\n                total_nan_features.append(col)\n            else:\n                partial_nan_features.append(col)\n    return no_nan_features, total_nan_features, partial_nan_features\n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "By playing around, I found out that id 711 belongs to a large group of ids that share the same features. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "no_nan, total_nan, partial_nan = find_nan_structure(711, df)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def find_id_group(instrument, data, lifetime_ids):\n    strong_cluster = []\n    soft_cluster = []\n    no_nan, total_nan, partial_nan = find_nan_structure(instrument, data)\n    no_nan_soft = set(no_nan).union(partial_nan)\n    for element_id in lifetime_ids:\n        no_nan_e, total_nan_e, partial_nan_e = find_nan_structure(element_id, data)\n        no_nan_soft_e = set(no_nan_e).union(partial_nan_e)\n        if set(no_nan_soft_e) == set(no_nan_soft):\n            soft_cluster.append(element_id)\n        if set(no_nan_e) == set(no_nan):\n            strong_cluster.append(element_id)\n    return strong_cluster, soft_cluster",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "strong_cluster, soft_cluster = find_id_group(711, df, long_lifetime_ids.id)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "strong_cluster",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I will proceed with the strong-cluster group of id 711. Let's create a dataframe which contains only these ids and let's try to find correlated features or do some other stuff. ;-) ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cluster_data = df[df[\"id\"].isin(strong_cluster)]\ncluster_data.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's select a feature, which does not contain nan-values, for playing around: ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_feature = no_nan[1]\nplt.figure()\nfor instrument in strong_cluster:\n    plt.plot(cluster_data[cluster_data[\"id\"]==instrument].timestamp, cluster_data[cluster_data[\"id\"]==instrument][test_feature].values, '.-')\nplt.xlabel(\"timestamp\")\nplt.ylabel(test_feature)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def find_id_groups(data, idlist, feature, limit):\n    groups = []\n    singles = []\n    for list_instrument in idlist:\n        group = []\n        for next_instrument in idlist:\n            coeff = np.corrcoef(data.ix[data.id==list_instrument, feature].values, data.ix[data.id==next_instrument, feature].values)[0,1]\n            coeff = np.round(coeff, decimals=2)\n            if coeff >= limit:\n                group.append(next_instrument)\n        for member in group:\n            while member in idlist:\n                idlist.remove(member)\n        if len(group) > 1:\n            groups.append(group)\n        elif len(group) == 1:\n            singles.append(list_instrument)\n    return groups, singles",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "id_list = strong_cluster[:]\ngroups, singles = find_id_groups(cluster_data, id_list, test_feature, 0.80)\ngroups",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Yeah! Found a pattern again! :-) Just by looking at \"stupid\" linear correlations, we can find that different features of one id are correlated but also that values of a specific feature of different ids are correlated in some cases. Let's have a look at those groups:  ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for group in groups:\n    plt.figure()\n    for instrument in group:\n        plt.plot(cluster_data[cluster_data.id==instrument][\"timestamp\"].values, cluster_data[cluster_data.id==instrument][test_feature].values, '.-')\n    plt.xlabel(\"timestamp\")\n    plt.ylabel(test_feature)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": ":-D",
      "metadata": {}
    }
  ]
}