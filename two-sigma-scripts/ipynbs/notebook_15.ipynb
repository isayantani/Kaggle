{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# A Basic Approach to Cluster Securities",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There is a large diversity of securities in the dataset. This probably makes building a single model for all id's very difficult. On the other hand, building a model for each of the securities individually is out of reach. Therefore it might be a good idea to seperate the securities into a couple of groups on which we can train a model jointy.\nIn the following, I am going to present a simple approach to find such a grouping.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Table of Contents**\n\n* Preparation\n* Visualisation\n* Finding Clusters\n* Exploring Cluster Characteristics\n* Conclusion",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Description**\n\nI do the clustering soley based on the first four moments of the asset returns (y) as I assume that these contain almost all of the relevant information that characterize different asset types. \n\nI neglect that certain securities are only available for a subperiod and may therefore have characteristics dominated by the temporary regime. In a further study the temporal component may be included in the clustering process, e.g. recluster every n timestep using the last m observations.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Preparation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "with pd.HDFStore('../input/train.h5') as train:\n    df = train.get('train')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "moments = df[['id', 'y']].groupby('id').agg([np.mean, np.std, stats.kurtosis, stats.skew]).reset_index()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "moments.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sec = moments['id']\ndat = moments['y']",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Scale the data\nfrom sklearn.preprocessing import StandardScaler\nscal = StandardScaler()\ndat = scal.fit_transform(dat)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Visualisation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dat = pd.DataFrame(dat)\ndat.columns = ['mean', 'std', 'kurtoris', 'skew']",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.pairplot(dat, size=1.5);",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are no well seperated clusters visible, but the plots nevertheless promise some kind diversity which we might be able to exploit.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Finding Clusters",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# determine suitable number of clusters with silhouette score\nk_max = 20\nk_range = range(2,k_max)\nscores = []\nfor k in k_range:\n    model = KMeans(n_clusters=k, n_init=20).fit(dat)\n    scores.append(silhouette_score(dat, model.labels_))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure()\nplt.plot(k_range, scores);\nplt.title('Results KMeans')\nplt.xlabel('n_clusters');\nplt.ylabel('Silhouette Score');",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This plot suggests that we should choose a k between 2 and 6. I will continue with a k=3.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "k = 3",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = KMeans(n_clusters=k).fit(dat)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "labels = pd.DataFrame([sec,model.labels_]).transpose()\nlabels.columns = ['sec', 'label']",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "labels.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create list of list such that members[i] contains id's with label i\nmembers = []\nfor i in range(k):\n    members.append(list(labels.sec[labels.label==i]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Size of each group: \", [len(_) for _ in members])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The small group is probably made up by the outliers we saw in the previous plot.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Exploring Cluster Characteristics",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "If the different clusters would indeed correspond to different asset classes, I would suspect that the available information for the different id's would reflect that, as bonds have different fundamentals as stocks or certain small caps might not report on all fundamentals that large caps do.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "prop_nan = pd.DataFrame(index=range(k), columns=df.columns[2:-1])\nfor i in range(k): #go through labels\n    dfi = df.loc[df['id'].isin(members[i]),:]\n    n = len(dfi)\n    for col in dfi.columns[2:-1]: #go through feature cols\n        prop_nan.set_value(i, col, dfi[col].isnull().sum() / n)\nfor col in df.columns[2:-1]: #go through feature cols\n    prop_nan.set_value('mean', col, df[col].isnull().sum() / len(df))\n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# only look at the features with significant missing data (>10%)\nprop_nan_sel = prop_nan.loc[:,prop_nan.loc['mean',:] > 0.1].reset_index()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "prop_nan_sel",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n = len(prop_nan_sel.columns[1:])\ncols = 3\nf, axarr = plt.subplots(int(n/cols)+1, cols, figsize=(8,30))\ni = 0\nfor col in prop_nan_sel.columns[1:]:\n    sns.barplot(x='index', y=col, data=prop_nan_sel,ax=axarr[int(i/cols),i%cols]);\n    axarr[int(i/cols),i%cols].set_ylabel('')\n    axarr[int(i/cols),i%cols].set_xlabel('')\n    i += 1",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Apparently our 'outlier class' has a significant amount of missing data. A reasonable explanation would be that these securities are penny stocks or the like which are highly volatile and don't have to publish the same data as blue chips do. \nWe can also observe that there are features for which either one of the two main classes, 0 or 1, have significantly more missing data than the other one. Maybe just confirmation bias (?) but is probably still worth digging deeper...",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Conclusion",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The previous elaboration showed a simple approach to using clustering techniques on the Two Sigma dataset. Despite its rudimentariness we could already observe some visible patterns which give enough motivation to do some more work down this path. I think that clustering could be a very powerful weapon to crack this dataset.\n\nThanks for reading! Any feedback is very much appreciated.\n\n\n\n**Potential further work:**\n\n* find better features to build clusters\n * maybe based on missing data\n* experiment with different number of clusters\n* try different models, e.g. Gaussian Mixture\n* do time dependent clustering to avoid influence of regime shifts",
      "metadata": {}
    }
  ]
}