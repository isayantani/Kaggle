{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "#Clustering on Missing Values\n\nIn a previous [notebook](https://www.kaggle.com/lesibius/two-sigma-financial-modeling/financial-instrument-types), I tried to see if I could differentiate among different instruments in the dataset using missing (or null) values. While my study was inconclusive, it was however clear that clustering this way could make sense (as suggested by the heatmap in the link above).\n\nThis said, I would like to go further in this direction, with these objectives in mind:\n<ul>\n<li>Find a way to keep my clusers constant. Using the k-means provide clusters in a random order, which prevents any further analysis. Building on my previous work, I intend to have a clear view of which features can be used in clustering.</li>\n<li>Analyse the features and the y variable within each cluster to see if it is likely to use a one-model-fits-all will work.</li>\n</ul>\n\nThe visualisation I obtained the last time suggests that two main clusters exist, and that some segregation could be done on the second cluster as well.\n\n# Results so Far\n<ul>\n<li>From the [previous notebook](https://www.kaggle.com/lesibius/two-sigma-financial-modeling/financial-instrument-types), I could isolate two main groups of ids based on their missing values</li>\n<li>One of the two groups contains more valid features that the other group. Overall, the cluster 0 has 57 \"mostly valid\" features, while the cluster 1 has only 24. 20 of these features are common to both clusters</li>\n<li>I unsuccessfully tried to apply a Kolmogorov-Smirnov two samples test on their y values to see if their distribution were roughly the same. However, it was inconclusive since it rejected the null hypothesis that both distribution were drawn from the same population while the four first moments and the cdf clearly indicate the contrary. For more information, see [this discussion](https://www.kaggle.com/c/two-sigma-financial-modeling/discussion/26406#) (thanks to CarrDelling and Oussama Errabia BTW) </li>\n</ul>\n\n# Recovering the Previous Results\n\nMy first goal is to recover the results of my previous kernel.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Importing libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n\n%matplotlib inline\n\n#Getting data\n\n#I owe this to SRK's work here:\n#https://www.kaggle.com/sudalairajkumar/two-sigma-financial-modeling/simple-exploration-notebook/notebook\nwith pd.HDFStore(\"../input/train.h5\", \"r\") as train:\n    df = train.get(\"train\")\n\n\ninput_variables = [x for x in df.columns.values if x not in ['id','y','timestamp']]\n\n\ndf_id_vs_variable = df[['id']+input_variables]       #Removes 'y' and 'timestamp'\ndf_id_vs_variable = df_id_vs_variable.fillna(0)      #Replace na by 0\n\ndef makeBinary(x):\n    if abs(x) > 0.00000:\n        return 1\n    else:\n        return 0\n\ndf_id_vs_variable = df_id_vs_variable.groupby('id').agg('sum').applymap(makeBinary)\n\n\nn_clust = 2\n\nkm = KMeans(n_clusters=n_clust, n_init=20).fit(df_id_vs_variable)\nclust = km.predict(df_id_vs_variable)\n\n\n#Init table of indexes\ndf_clust_index = {}\nfor i in range(0,n_clust):\n    df_clust_index[i]=[]\n\n#Fill the cluster index\nfor i in range(0,len(clust)):\n    df_clust_index[clust[i]].append(i)\n\nfor i in range(0,n_clust):\n    df_clust_index[i] = df_id_vs_variable.iloc[df_clust_index[i]].index.values\n\n\n\ndf_clust = []\n\nfor i in range(0,n_clust):\n    df_clust.append(df.loc[df.id.isin(df_clust_index[i])])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "At this stage, we have some useful variables:\n\n<ul>\n <li>`df_id_vs_variable` which provides a binary matrix (1: non-null variable for the id, 0: null-variable) </li>\n<li>`clust` which is the result of the skleran kmean (i.e. an array containing the cluster number for each observation)</li>\n<li>`df_clust_index` which has the following shape: {index_clust_0:[ids in cluster 0], index_clust_n:[ids in cluster n]}</li>\n<li>`df_clust`: an array of dataframe, where the index of the array represent the cluster number</li>\n</ul>\n\n# Two Clusters Analysis\n\nA first step is to see whether the data from the two clusters seems to be drawn from the same population.\n\n## Null vs Non-Null Variables\n\nFirst, I \"serialize\" my clusters. At the same time, I would like to keep only columns which exhibit sufficient data to be used. Thus, I only keep these clusters if 95% of the ids have a value.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "non_null_0 = df_id_vs_variable.loc[clust==0].sum() / df_id_vs_variable.loc[clust==0].shape[0]\nnon_null_1 = df_id_vs_variable.loc[clust==1].sum() / df_id_vs_variable.loc[clust==1].shape[0]\n\n\ndf_non_null_comparison = pd.concat([non_null_0,non_null_1],axis=1)\n\nbar_width = 1\nindex = np.arange(df_non_null_comparison.shape[0])\n\nfig, ax = plt.subplots(figsize=(12,50))\n\nrects1 = plt.barh(index ,  np.array(df_non_null_comparison[0]), bar_width/2,\n                 color='b',\n                 label='Cluster 0')\n\nrects1 = plt.barh(index + bar_width/2,  np.array(df_non_null_comparison[1]), bar_width/2,\n                 color='r',\n                 label='Cluster 1')\n\n\n#plt.figure(figsize=(20,50))\nplt.legend()\nplt.xlabel('Percentage of Null-Values')\nplt.ylabel('Features')\nplt.yticks(index + bar_width, df_non_null_comparison.index.values)\nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "non_null_threshold = 0.95\n\ncol_0 = non_null_0.loc[non_null_0 > non_null_threshold].index.values\ncol_1 = non_null_1.loc[non_null_1 > non_null_threshold].index.values",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The following lists contains \"serialised\" column names.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "col_0 = ['derived_2', 'fundamental_0', 'fundamental_2', 'fundamental_7', 'fundamental_8', 'fundamental_10', 'fundamental_11', 'fundamental_13', 'fundamental_14', 'fundamental_15', 'fundamental_16', 'fundamental_18', 'fundamental_19', 'fundamental_21', 'fundamental_23', 'fundamental_29', 'fundamental_30', 'fundamental_33', 'fundamental_35', 'fundamental_36', 'fundamental_37', 'fundamental_39', 'fundamental_41', 'fundamental_42', 'fundamental_43', 'fundamental_44', 'fundamental_45', 'fundamental_46', 'fundamental_48', 'fundamental_50', 'fundamental_53', 'fundamental_54', 'fundamental_55', 'fundamental_56', 'fundamental_59', 'fundamental_60', 'fundamental_62', 'technical_1', 'technical_2', 'technical_3', 'technical_6', 'technical_7', 'technical_11', 'technical_13', 'technical_17', 'technical_19', 'technical_20', 'technical_21', 'technical_22', 'technical_24', 'technical_27', 'technical_30', 'technical_33', 'technical_35', 'technical_36', 'technical_40', 'technical_41']\ncol_1 = ['technical_1', 'technical_2', 'technical_3', 'technical_5', 'technical_6',\n     'technical_7', 'technical_11', 'technical_13', 'technical_14', 'technical_17',\n     'technical_19', 'technical_20', 'technical_21', 'technical_22', 'technical_24',\n     'technical_27', 'technical_30', 'technical_33', 'technical_34', 'technical_35',\n     'technical_36', 'technical_40', 'technical_41', 'technical_43']\n\ncommon_cols = [x for x in col_0 if x in col_1]\n\nprint(\"Cluster 0 has {0} columns\".format(len(col_0)))\nprint(\"Cluster 1 has {0} columns\".format(len(col_1)))\n\nprint(\"Number of features present in both clusters: {0}\".format(len(common_cols)))\nprint(\"Number of features that are only present in cluster 0: {0}\".format(len([x for x in col_0 if x not in col_1])))\nprint(\"The following features are only present in cluster 1: {0}\".format(len([x for x in col_1 if x not in col_0])))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Kolmogorov-Smirnov Analysis on the 'y' Value\n\nThis section and the following were an unfruitful try to see if the distribution of y values among clusters were different. As the graphs and first four moments exhibit the contrary of the test result, I will just let this part as is for the record.\n\n*Null hypothesis*  - H0: the two clusters are drawn from the same distribution.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy import stats\n\ny_0 = df_clust[0].y.dropna().values\ny_1 = df_clust[1].y.dropna().values \n\nprint(\"{:01.3f}\".format(stats.ks_2samp(y_0, y_1)[1]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We reject the null hypothesis at the 5% threshold. The two clusters apparently result from different distributions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nfor i in range(0,n_clust):\n    n, bins, patches = plt.hist(df_clust[i].y.dropna().values, 50, normed=1, facecolor='green', alpha=0.75)\n    plt.xlabel('y Value')\n    plt.ylabel('Occurence')\n    plt.title(r'Distribution of y Value for Cluster '+str(i))\n    plt.show()\n    print(\"Mean value: {:.3e}\".format(df_clust[i].y.dropna().mean()))\n    print(\"Standard deviation: {:.3e}\".format(df_clust[i].y.dropna().std()))\n    print(\"Median value: {:.3e}\".format(df_clust[i].y.dropna().median()))\n    print(\"Skew: {:.3e}\".format(df_clust[i].y.dropna().skew()))\n    print(\"Kurtosis: {:.3e}\".format(df_clust[i].y.dropna().kurtosis()))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Kolmogorov-Smirnov Analysis on the Common Features",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "p_values_y = map(lambda x: stats.ks_2samp(df_clust[0][x].dropna().values, df_clust[1][x].dropna().values)[1],common_cols)\n\ndef isrejected(pval,th = 0.05):\n    if pval < th:\n        return \"rejected\"\n    else:\n        return \"not rejected\"\nfor pv in list(p_values_y):\n    print(\"{:.3e}: {}\".format(pv,isrejected(pv)))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_clust[1].shape",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Inter-Cluster Features Correlation\n\nI would like to see now if some correlations exist between the features that are common to both clusters and those that are related to only one of them. The idea would be to use some models to fill missing values based on the non-empty data, in order to keep more features (or to avoid to fill them with their averages).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#This code's purpose is to make sure that the cluster 0 in my previous analysis is the same here.\n\nif(non_null_0.loc[non_null_0 > non_null_threshold].index.isin(col_0).sum() < 24):\n    temp_df = df_clust[0]\n    df_clust[0] = df_clust[1]\n    df_clust[1] = temp_df",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cl=0\ncols=[col_0,col_1]\nfor cl in [0,1]:\n    remaining_cols = [x for x in cols[cl] if x not in cols[1-cl]]\n\n    df_corr = df_clust[cl][common_cols + remaining_cols]\n    df_corr = df_corr.corr()\n\n    df_corr = df_corr[common_cols].loc[df_corr.index.isin(remaining_cols)]\n\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    f, ax = plt.subplots(figsize=(11, 9))\n    sns.heatmap(df_corr, cmap=cmap, vmax=1,\n                square=True, xticklabels=True, yticklabels=True,\n                linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "From the cluster 0, it might be possible to create a model on 'fundamental_21', 'fundamental_54' and 'fundamental_60'. From the cluster 1, 'technical_14' and 'technical_43'.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}