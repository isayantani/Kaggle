{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This notebook is trying to create new features from the features given",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport xgboost as xgb\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain = pd.read_json(\"../input/train.json\")\ntest = pd.read_json(\"../input/train.json\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train['source']='train'\ntest['source']='test'\nprint(train.head())\nprint(test.info())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Handling Bathrooms and bedrooms",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(train['bathrooms'].value_counts())\n# There are houses with 0.0 bathrooms and some with floating point no of bathrooms\nprint(train['bedrooms'].value_counts())\n# There are 9475 houses with 0 bedrooms\ntrain['bathrooms']=train['bathrooms'].astype(int)\ntest['bathrooms']=test['bathrooms'].astype(int)\n# if no of bathrooms are greater than 5 interest level is low else varies\n# so we can create dummies\ntrain.loc[train['bathrooms']==0, 'interest_level'] = 'low'\nsns.violinplot(x='interest_level', y='bathrooms', data=train)\nplt.xlabel('Interest level')\nplt.ylabel('bathrooms')\nplt.show()\n# if 0 or greater than 4 bathrooms interest_level is low",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Creating new variables from bathrooms",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"price_tt\"] =train[\"price\"]/train[\"bathrooms\"]\ntest[\"price_tt\"] = test[\"price\"]/test[\"bathrooms\"] \n\ntrain[\"room_sum\"] = train[\"bedrooms\"]+train[\"bathrooms\"] \ntrain[\"room_sum\"] = test[\"bedrooms\"]+test[\"bathrooms\"] ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Bedrooms",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# same patern as bathrooms only different at 0\n# pattern between bedrooms and  bathrooms\nprint(train.loc[train['bathrooms']==0, 'bedrooms'].value_counts())\nprint(train.loc[train['bathrooms']==6, 'bedrooms'].value_counts())\n# we can combine 6 or more #bathrooms with 5 or more bedrooms\nsns.violinplot(x='interest_level', y='bedrooms', data=train)\nplt.xlabel('Interest level')\nplt.ylabel('bedrooms')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Adding variables from bedrooms",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"price_t\"] =train[\"price\"]/train[\"bedrooms\"]\ntest[\"price_t\"] = test[\"price\"]/test[\"bedrooms\"] \n\ntrain['room_sum'] = train['bedrooms']  + train['bathrooms']\ntest['room_sum'] = test['bedrooms']  + test['bathrooms']\ntrain['price_per_room'] = train['price']/train['room_sum']\ntest['price_per_room'] = test['price']/test['room_sum']",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Handling time (created)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"created\"] = pd.to_datetime(train[\"created\"])\ntrain[\"created_year\"] = train[\"created\"].dt.year\ntrain[\"created_month\"] = train[\"created\"].dt.month\ntrain[\"created_day\"] = train[\"created\"].dt.day\ntrain[\"created_hour\"] = train[\"created\"].dt.hour\ntest[\"created\"] = pd.to_datetime(test[\"created\"])\ntest[\"created_year\"] = test[\"created\"].dt.year\ntest[\"created_month\"] = test[\"created\"].dt.month\ntest[\"created_day\"] = test[\"created\"].dt.day\ntest[\"created_hour\"] = test[\"created\"].dt.hour",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Log of Price as it is right skewed",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.scatter(range(train.shape[0]), np.sort(train.price.values))\nplt.xlabel('index')\nplt.ylabel('price')\nplt.show()\n# there are outliners\nulimit = np.percentile(train.price.values, 99)\ntrain['price'].ix[train['price']>ulimit] = ulimit\n# price is right skewed so using log to create a gaussian pattern\ntrain['price']=np.log1p(train['price'])\ntest['price']=np.log1p(test['price'])\n\nplt.figure(figsize=(8,6))\nsns.distplot(train.price.values, bins=50, kde=True)\nplt.xlabel('price')\nplt.show()\nsns.violinplot(data=train,x = 'interest_level',y='price')\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Street address and Display address",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder\ndisplay_count = train.groupby('display_address')['display_address'].count()\nplt.hist(display_count.values, bins=100, log=True, alpha=0.9)\nplt.xlabel('Number of times display_address appeared', fontsize=12)\nplt.ylabel('log of Count', fontsize=12)\nplt.show()\n# there are too many values and none of them are more than 500\n# most of the values are less than 10\n#so we label encode the values\naddress = [\"display_address\", \"street_address\"]\nfor x in address:\n    le = LabelEncoder()\n    le.fit(list(train[x].values))\n    train[x] = le.transform(list(train[x].values))\n    le.fit(list(test[x].values))\n    test[x] = le.transform(list(test[x].values))\n    ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Find position and neighbourhood from latitude and longitude",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"pos\"] = train.longitude.round(3).astype(str) + '_' + train.latitude.round(3).astype(str)\ntest[\"pos\"] = test.longitude.round(3).astype(str) + '_' + test.latitude.round(3).astype(str)\n\nfrom sklearn.cluster import Birch\ndef cluster_latlon(n_clusters, data):  \n    print(\"data.shape\")\n    print(data.shape)\n    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    print(data_c.shape)\n    data_e=data[~((data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9))]\n    print(data_e.shape)\n    #put it in matrix form\n    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n    \n    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n\n    brc.fit(coords)\n    clusters=brc.predict(coords)\n    data_c[\"cluster_\"+str(n_clusters)]=clusters\n    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n    data=pd.concat([data_c,data_e])\n    print(data.shape)\n    #plt.scatter(data_c[\"longitude\"], data_c[\"latitude\"], c=data_c[\"cluster_\"+str(n_clusters)], s=10, linewidth=0.1)\n    #plt.title(str(n_clusters)+\" Neighbourhoods from clustering\")\n    #plt.show()\n    return data \n\ntrain=cluster_latlon(100, train)\nprint(train.head())\nclusters_price_map=dict(train.groupby(by=\"cluster_100\")[\"price\"].median())\ntrain[\"price_comparison\"]=train['price']-train[\"cluster_100\"].map(clusters_price_map)\n\ntest=cluster_latlon(100, test)\n\nclusters_price_map=dict(test.groupby(by=\"cluster_100\")[\"price\"].median())\ntest[\"price_comparison\"]=test['price']-test[\"cluster_100\"].map(clusters_price_map)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "manager_id",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(len(train['manager_id'].unique()))\n# 3481 unique managers\ntemp = train.groupby('manager_id').count().iloc[:,-1]\ntemp2 = test.groupby('manager_id').count().iloc[:,-1]\ntrain_managers = pd.concat([temp,temp2],axis=1,join='outer')\ntrain_managers.columns=['train_count','test_count']\nprint(train_managers.sort_values(by = 'train_count',ascending = False).head())\n# considering only those manager_ids which are in train\nman_list = train_managers['train_count'].sort_values(ascending = False).head(3481).index\nixes = train.manager_id.isin(man_list)\ntrain10 = train[ixes][['manager_id','interest_level']]\n# create dummies of interest levels\ninterest_dummies = pd.get_dummies(train10.interest_level)\ntrain10 = pd.concat([train10,interest_dummies[['low','medium','high']]], axis = 1).drop('interest_level', axis = 1)\nprint(train10.head())\ngby = pd.concat([train10.groupby('manager_id').mean(),train10.groupby('manager_id').count()], axis = 1).iloc[:,:-2]\ngby.columns = ['low','medium','high','count']\ngby.sort_values(by = 'count', ascending = False).head(10)\ngby['manager_skill'] = gby['medium']*1 + gby['high']*2 \ngby['manager_id']=gby.index\nprint(gby.head(5))\nprint(gby.shape)\ntrain = train.merge(gby[['manager_id','manager_skill']],on='manager_id',how='outer',right_index=False)\ntrain['manager_skill']=train['manager_skill'].fillna(0)\nprint(train.head())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Adding interest acc to manager_id to test acc to train data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "index=list(range(train.shape[0]))\nrandom.shuffle(index)\na=[np.nan]*len(train)\nb=[np.nan]*len(train)\nc=[np.nan]*len(train)\n\nfor i in range(5):\n    building_level={}\n    for j in train['manager_id'].values:\n        building_level[j]=[0,0,0]\n    \n    test_index=index[int((i*train.shape[0])/5):int(((i+1)*train.shape[0])/5)]\n    train_index=list(set(index).difference(test_index))\n    \n    for j in train_index:\n        temp=train.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n            \n    for j in test_index:\n        temp=train.iloc[j]\n        if sum(building_level[temp['manager_id']])!=0:\n            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n            \ntrain['manager_level_low']=a\ntrain['manager_level_medium']=b\ntrain['manager_level_high']=c\n\na=[]\nb=[]\nc=[]\nbuilding_level={}\nfor j in train['manager_id'].values:\n    building_level[j]=[0,0,0]\n\nfor j in range(train.shape[0]):\n    temp=train.iloc[j]\n    if temp['interest_level']=='low':\n        building_level[temp['manager_id']][0]+=1\n    if temp['interest_level']=='medium':\n        building_level[temp['manager_id']][1]+=1\n    if temp['interest_level']=='high':\n        building_level[temp['manager_id']][2]+=1\n\nfor i in test['manager_id'].values:\n    if i not in building_level.keys():\n        a.append(np.nan)\n        b.append(np.nan)\n        c.append(np.nan)\n    else:\n        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n        c.append(building_level[i][2]*1.0/sum(building_level[i]))\ntest['manager_level_low']=a\ntest['manager_level_medium']=b\ntest['manager_level_high']=c\nprint(train.head())\nprint(test.head())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Adding interest level  Building_id decreases the model accuracy",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Adding no of photos , no of words in features and description",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train[\"num_photos\"] = train[\"photos\"].apply(len)\ntest[\"num_photos\"] = test[\"photos\"].apply(len)\n\ntrain[\"num_features\"] = train[\"features\"].apply(len)\ntest[\"num_features\"] = test[\"features\"].apply(len)\n\ntrain[\"num_description_words\"] = train[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest[\"num_description_words\"] = test[\"description\"].apply(lambda x: len(x.split(\" \")))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Changing features to sparse matrix",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train['features'] = train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest['features'] = test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train[\"features\"])\nte_sparse = tfidf.transform(test[\"features\"])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}