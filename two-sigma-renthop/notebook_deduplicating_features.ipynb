{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Processing and deduplicating Features",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import defaultdict, Counter",
      "execution_count": 28,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_df = pd.read_json(\"../input/train.json\")\ntest_df = pd.read_json(\"../input/test.json\")\ntrain_test = pd.concat([train_df, test_df], 0)",
      "execution_count": 29,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I'm going do assume you've taken a look at the data and noticed that some features are duplicates and that some are also rare (occurs very few times).\n\nThat being said lets try to drop rare features and also try to deduplicate similar features using first-k-chars. as a hash.\n\nFirst we'll read in data, lowercase everything and remove any whitespace",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features = train_test[[\"features\"]].apply(\n    lambda _: [list(map(str.strip, map(str.lower, x))) for x in _])",
      "execution_count": 30,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features.head()",
      "execution_count": 31,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# count features and drop features with less than n counts\n\nn = 5\n\nfeature_counts = Counter()\nfor feature in features.features:\n    feature_counts.update(feature)\nfeature = sorted([k for (k,v) in feature_counts.items() if v > n])\nfeature[:10]",
      "execution_count": 32,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\nNotice that we see like variations of `24-hour` and we will see later that thats not the only duplicate features ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Hashing cleaned up data using first4 characters.\n\nWe will first do some manual work to simplify the strings and use the first four characters as a key for each feature. then we will use that key to deduce data.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def clean(s):\n    x = s.replace(\"-\", \"\")\n    x = x.replace(\" \", \"\")\n    x = x.replace(\"twenty four hour\", \"24\")\n    x = x.replace(\"24/7\", \"24\")\n    x = x.replace(\"24hr\", \"24\")\n    x = x.replace(\"24-hour\", \"24\")\n    x = x.replace(\"24hour\", \"24\")\n    x = x.replace(\"24 hour\", \"24\")\n    x = x.replace(\"common\", \"cm\")\n    x = x.replace(\"concierge\", \"doorman\")\n    x = x.replace(\"bicycle\", \"bike\")\n    x = x.replace(\"private\", \"pv\")\n    x = x.replace(\"deco\", \"dc\")\n    x = x.replace(\"decorative\", \"dc\")\n    x = x.replace(\"onsite\", \"os\")\n    x = x.replace(\"outdoor\", \"od\")\n    x = x.replace(\"ss appliances\", \"stainless\")\n    return x\n\ndef feature_hash(x):\n    cleaned = clean(x, uniq)\n    key = cleaned[:4].strip()\n    return key",
      "execution_count": 33,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "key2original = defaultdict(list)\nk = 4\nfor f in feature:\n    cleaned = clean(f)\n    key = cleaned[:k].strip()\n    key2original[key].append(f)",
      "execution_count": 34,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Lets take a look at the dedups! Don't worry about the key, but just take a look at what values are in the same key",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "key2original",
      "execution_count": 35,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"number of deduped features:\", len(key2original))\nprint(\"number of old features:\", len(feature))",
      "execution_count": 36,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In order to make this easier to use, I'll output this as a CSV of the original feature and the deduced string.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def to_tuples():\n    for f in feature:\n        key = clean(f)[:k].strip()\n        yield (f, key2original[key][0])\n        \ndeduped = list(to_tuples())\ndf = pd.DataFrame(deduped, columns=[\"original_feature\", \"unique_feature\"])",
      "execution_count": 45,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": 46,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df.to_csv(\"feature_deduplication.csv\", index=False)",
      "execution_count": 47,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}