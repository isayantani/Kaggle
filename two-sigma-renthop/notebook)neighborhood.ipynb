{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "In this notebook I show two methods to encode location features (longitude, latitude) to get a \"neighborhood\" proxy feature.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom sklearn.cluster import KMeans",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Load dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_json('../input/train.json')[['longitude','latitude','interest_level','price']]\ndf['response'] = 0.\ndf.loc[df.interest_level=='medium', 'response'] = 0.5\ndf.loc[df.interest_level=='high', 'response'] = 1",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Outlier removal\n\nIf you take a look to the location coordinates you'll notice a few outliers far away from NY city center. I'll start by excluding them using a recursive approach. At each iteration all the listings with |z-score|>3 are excluded. The removal algorithm stops once all the listings have |z-score|<3.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Show location coordinates before oulier removal\nfig, ax = plt.subplots(1, 2, figsize=(9,6))\nprint('Length before removing ouliers', len(df))\nax[0].plot(df.longitude, df.latitude, '.');\nax[0].set_title('Before outlier removal');\nax[0].set_xlabel('Longitude');\nax[0].set_ylabel('Latitude');\n# Outlier removal\nfor i in ['latitude', 'longitude']:\n    while(1):\n        x = df[i].median()\n        ix = abs(df[i] - x) > 3*df[i].std()\n        if ix.sum()==0: # no more outliers -> stop\n            break\n        df.loc[ix, i] = np.nan # exclude outliers\n# Keep only non-outlier listings\ndf = df.loc[df[['latitude', 'longitude']].isnull().sum(1) == 0, :]\nprint('Length after removing ouliers', len(df))\n# Show location coordinates after outlier removal\nax[1].plot(df.longitude, df.latitude, '.');\nax[1].set_title('After outlier removal');\nax[1].set_xlabel('Longitude');\nax[1].set_ylabel('Latitude');",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Neighborhoods\nHere, I'll use Kmeans to cluster listings based on their coordinates. I'll show the results of using 5, 10, 20 and 40 \"neighborhoods\".",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(4,1,figsize=(9,30))\nfor ix, ncomp in enumerate([5, 10, 20, 40]):\n    r = KMeans(ncomp, random_state=1)\n    # Normalize (longitude, latitude) before K-means\n    temp = df[['longitude', 'latitude']].copy()\n    temp['longitude'] = (temp['longitude']-temp['longitude'].mean())/temp['longitude'].std()\n    temp['latitude'] = (temp['latitude']-temp['latitude'].mean())/temp['latitude'].std()\n    # Fit k-means and get labels\n    r.fit(temp[['longitude', 'latitude']])\n    df['labels'] = r.labels_\n    # Plot results\n    cols = sns.color_palette(\"Set2\", n_colors=ncomp, desat=.5)\n    cl = [cols[i] for i in r.labels_]\n    area = 12\n    ax[ix].scatter(df.longitude, df.latitude, s=area, c=cl, alpha=0.5);\n    ax[ix].set_title('Number of components: ' + str(ncomp))\n    ax[ix].set_xlabel('Longitude')\n    ax[ix].set_ylabel('Latitude')\n    # Show aggregated volume and interest at each neighborhood\n    x = df.groupby('labels')[['longitude','latitude','response']].mean().sort_values(['response'])\n    x = pd.concat([x, df['labels'].value_counts()], axis=1).sort_values(['response'])\n    cols = sns.color_palette(\"RdBu_r\", ncomp)[::-1]\n    for i in range(ncomp):\n        props = dict(boxstyle='round', facecolor=cols[i], alpha=0.8)\n        ax[ix].text(x.longitude.values[i], x.latitude.values[i], \n                str(np.array(np.round(x.response.values,2), '|S8')[i])+'\\n'+str(np.array(x['labels'].values, '|S8')[i]), \n                fontsize=9, verticalalignment='center', horizontalalignment='center', bbox=props);",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Supervised encoding of location coordinates\nThe aim of this analysis is to provide a way to numericaly encode listing locations based on the mean location interest. I'll proceed as follows:\n\n- Define a fixed grid of 40 points both for 'latitude' and 'longitude' values\n- At each square defined by two consecutive longitude and latitude breaks compute the mean interest of the listings within the square",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "res = 40 # grid size\nmin_n = 30 # minimum size to perform inference\n# Define grids\nnx = np.linspace(df.longitude.min(), df.longitude.max(), res)\nny = np.linspace(df.latitude.min(), df.latitude.max(), res)\n# Encode\nY = pd.DataFrame()\nfor i in range(res-1):\n    for j in range(res-1):\n        # Identify listings within the square\n        ix = (df.longitude >= nx[i])&(df.longitude < nx[i+1])&(df.latitude >= ny[j])&(df.latitude < ny[j+1])\n        # Compute mean interest if the number of listings is greated than 'min_n'\n        if ix.sum() > min_n:\n            y = df.loc[ix, :].mean() # mean interest\n            y['n'] = ix.sum() # volume\n            Y = pd.concat([Y, y], axis=1)\nY = Y.transpose()\n# Show results\ncols = sns.color_palette(\"RdBu_r\", 5)\ncl = [cols[i] for i in np.digitize(Y.response.values, Y.response.quantile(np.arange(1/5., 1, 1/5.)))]\narea = 12 + 5*np.log1p(Y.n - min_n)\nfig, ax = plt.subplots(1, 1, figsize=(9,9))\nax.scatter(Y.longitude, Y.latitude, s=area, c=cl, alpha=0.5);",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Any feedback or comment will be appreciated! Upvote if you found it interesting :)\n\nThanks!",
      "metadata": {}
    }
  ]
}