{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "text classification try",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\n\n# data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import euclidean_distances\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_rent= pd.read_json(\"../input/train.json\")\ndata_rent.head()\nprint(data_rent.groupby('interest_level').size())",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_rent.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_rent['features']=data_rent['features'].apply(lambda x: ', '.join(x))\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndef cleaning_text(sentence):\n   sentence=sentence.lower()\n   sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n   sentence=re.sub('\\d+',' ', sentence) #removes digits\n   cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n   cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n   #selecting only nouns and adjectives\n   cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n   cleaned=cleaned.strip()\n   return cleaned\n\t  \ndata_rent['cleaned']= data_rent['description'].apply(lambda x: cleaning_text(x))\ndata_rent['feat_cleaned']= data_rent['features'].apply(lambda x: cleaning_text(x))\ndata_rent[\"final_feat\"] = data_rent[\"cleaned\"].map(str) +\" \"+data_rent[\"feat_cleaned\"]\ndata_rent.head(2)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_extraction import DictVectorizer as DV\nvectorizer = DV( sparse = False )\ndata_rent_1 = vectorizer.fit_transform(data_rent)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.distplot(data_rent['bedrooms'],kde=False)\n#sns.distplot(data_rent['bathrooms'],kde=False)\n#data_rent_1=data_rent.get_dummies(columns='interest_level')\ndata_rent['interest_level'] =data_rent['interest_level'].astype('category')\ndata_rent['interest_level_cat']=data_rent['interest_level'].cat.codes",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#plt.plot(data_rent['interest_level_cat'],data_rent['bathrooms'])\nplt.scatter(data_rent['interest_level_cat'],data_rent['bathrooms'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\ngrouped = data_rent.groupby('color')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='hist', x='interest_level_cat', y='bathrooms', label=key, color=colors[key])\n\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_high=data_rent.loc[(data_rent['interest_level']=='high')]\ndata_medium=data_rent.loc[(data_rent['interest_level']=='medium')]\ndata_low=data_rent.loc[(data_rent['interest_level']=='low')]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_high['cleaned'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data_rent, test_size = 0.2)\nprint(len(train))\nprint(len(test))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "binVectorizer = CountVectorizer(binary=True)\ncounts = binVectorizer.fit_transform(train['cleaned'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\ntargets = train['interest_level'].values\nclassifier.fit(counts, targets)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "examples = test['cleaned']\nexample_counts = binVectorizer.transform(examples)\npredictions = classifier.predict(example_counts)\npredictions_df=pd.DataFrame(predictions)\npredictions_df.head(10)\nactual=test['interest_level'].values\nfrom sklearn.metrics import confusion_matrix\nmatrix=pd.DataFrame(confusion_matrix(actual, predictions,labels=[\"low\", \"medium\", \"high\"]))\nprint(matrix)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pd.crosstab(test['interest_level'], predictions, rownames=['True'], colnames=['Predicted'], margins=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('vectorizer',  CountVectorizer()),\n    ('classifier',  MultinomialNB()) ])\n\n ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipeline.fit(train['cleaned'].values, train['interest_level'].values)\npredicts=pipeline.predict(test['cleaned'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_test= pd.read_json(\"../input/test.json\")\ndata_test.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}