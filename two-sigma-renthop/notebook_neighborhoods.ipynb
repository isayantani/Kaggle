{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This an idea on how to use lat lon data to create a new variable. At first I wanted to use NYC's districts polygon map to derive a district appartenance for each listing_id but that would have been using external data. So instead of this I figured I could just derive natural district from the lat long data using a little clustering. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline \npd.options.mode.chained_assignment = None  # default='warn'",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train=pd.read_json(\"../input/train.json\")\ntest=pd.read_json(\"../input/test.json\")\ntrain[\"Source\"]='train'\ntest[\"Source\"]='test'\ndata=pd.concat([train, test]) ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.scatter(data[\"longitude\"], data[\"latitude\"], s=5)\nplt.title(\"Geographical positions of the listings\")\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Geographic plotting ##",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Clearly, it seems that some of the data is missing (0,0): unless there are some people are looking for an appartment right in the guinea gulf.  Let's remove them so we can have a better view of the data. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.scatter(data.loc[data[\"longitude\"]<-60,\"longitude\"], data.loc[data[\"latitude\"]>20,\"latitude\"], s=5)\nplt.title(\"Geographical positions of the listings\")\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "There are a few flats all around the US but most of the cloud is around NYC\nSo lets zoom in on NYC ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.scatter(data.loc[(data[\"longitude\"]<-73.75)&(data[\"longitude\"]>-74.05)&(data[\"latitude\"]>40.4)&(data[\"latitude\"]<40.9),\"longitude\"],\n                      data.loc[(data[\"latitude\"]>40.4)&(data[\"latitude\"]<40.9)&(data[\"longitude\"]<-73.75)&(data[\"longitude\"]>-74.05),\"latitude\"], s=5)\nplt.title(\"Geographical positions of the listings\")\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "At this level there are enough points to see some known features: we see the shape of manhattan with a hole for central park for example. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Clustering NYC data ##",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n\n#I use Birch because of how fast it is. \nfrom sklearn.cluster import Birch\ndef cluster_latlon(n_clusters, data):  \n    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    data_e=data[~((data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9))]\n    #put it in matrix form\n    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n    \n    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n\n    brc.fit(coords)\n    clusters=brc.predict(coords)\n    data_c[\"cluster_\"+str(n_clusters)]=clusters\n    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n    data=pd.concat([data_c,data_e])\n    plt.scatter(data_c[\"longitude\"], data_c[\"latitude\"], c=data_c[\"cluster_\"+str(n_clusters)], s=10, linewidth=0.1)\n    plt.title(str(n_clusters)+\" Neighbourhoods from clustering\")\n    plt.show()\n    return data ",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cluster_latlon(50, data)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The ideal algorithm for this would be DBSCAN as shown in here http://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/ however it is too heavy to run on this kernel because I believe it has to compute a matrix of distance between each points. The advantage of DBSCAN is that it would leave the \"extremum\" point out of the clusters while Birch here is creating clusters of very low density for those ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Naive variables##",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data[\"created\"]=pd.to_datetime(data[\"created\"])\ndata[\"created_month\"]=data[\"created\"].dt.month\ndata[\"created_day\"]=data[\"created\"].dt.day\ndata[\"created_hour\"]=data[\"created\"].dt.hour",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data[\"num_photos\"]=data[\"photos\"].apply(len)\ndata[\"num_features\"]=data[\"features\"].apply(len)\ndata[\"num_description_words\"] = data[\"description\"].apply(lambda x: len(x.split(\" \")))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "features_to_use  = [\"bathrooms\", \"bedrooms\", \"price\", \n                                                     \n                    \"num_photos\", \"num_features\", \"num_description_words\",                    \n                    \"created_month\", \"created_day\", \"created_hour\"\n                   ]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train=data[data[\"Source\"]==\"train\"]\ntest=data[data[\"Source\"]==\"test\"]\ntarget_num_map={\"high\":0, \"medium\":1, \"low\":2}\ny=np.array(train[\"interest_level\"].apply(lambda x: target_num_map[x]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_val,y_train, y_val =train_test_split( train[features_to_use], y, test_size=0.33, random_state=42)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier as RFC\nclf=RFC(n_estimators=1000, random_state=42)\nclf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import log_loss\ny_val_pred = clf.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Adding the cluster variable ##",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import log_loss\n\n\ndef compute_logloss(n_cluster,data):\n    data_cluster=cluster_latlon(n_cluster,data)\n    train=data_cluster[data_cluster[\"Source\"]==\"train\"]\n\n    target_num_map={\"high\":0, \"medium\":1, \"low\":2}\n    y=np.array(train[\"interest_level\"].apply(lambda x: target_num_map[x]))\n    \n    features = [\"bathrooms\", \"bedrooms\", \"price\", \n                                                        \n                    \"num_photos\", \"num_features\", \"num_description_words\",                    \n                    \"created_month\", \"created_day\", \"created_hour\", \"cluster_\"+str(n_cluster)\n                   ]\n    \n    X_train, X_val,y_train, y_val =train_test_split( train[features], y, test_size=0.33, random_state=42)\n    clf.fit(X_train, y_train)\n\n    y_val_pred = clf.predict_proba(X_val)\n    return log_loss(y_val, y_val_pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "compute_logloss(3, data)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A tiny bit better but lets check with more clusters",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "log_loss_cls={}\nfor n in range(4,15):\n    log_loss_cls[n]=compute_logloss(n, data)\n    \nn_c = sorted(log_loss_cls.items()) \nx, y = zip(*n_c) \nplt.plot(x, y)\nplt.title(\"log_loss for different numbers of clusters\")\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "log_loss_cls",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "It seems that the more clusters, the better the log_loss becomes. A the extreme of this each point is his own cluster and we are back to the lat ,lon original data",
      "metadata": {}
    }
  ]
}