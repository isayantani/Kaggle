{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "This notebook shows how you can use description to improve your model. We will be using description, as the only feature for now.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train = pd.read_json(\"../input/train.json\")\ntest = pd.read_json(\"../input/test.json\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# We need listing_id, description and interest_level for this notebook\ntrain = train[['listing_id','description','interest_level']]\ntest = test[['listing_id','description']]\n\ntrain['flag'] = 'train'\ntest['flag'] = 'test'\nfull_data = pd.concat([train,test])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from nltk.stem import PorterStemmer\nimport re",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "> Stemming is the process of reducing inflected (or sometimes derived) words to their word stem. Example: gardens to garden.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Removes symbols, numbers and stem the words to reduce dimentional space\nstemmer = PorterStemmer()\n\ndef clean(x):\n    regex = re.compile('[^a-zA-Z ]')\n    # For user clarity, broken it into three steps\n    i = regex.sub(' ', x).lower()\n    i = i.split(\" \") \n    i= [stemmer.stem(l) for l in i]\n    i= \" \".join([l.strip() for l in i if (len(l)>2) ]) # Keeping words that have length greater than 2\n    return i",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# This takes some time to run. It would be helpful if someone can help me optimize clean() function.\nfull_data['description_new'] = full_data.description.apply(lambda x: clean(x))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "full_data[['description','description_new']].head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We have removed all punctuation and numbers, as we are only interested in words for now.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Using CountVectorizer\nWe can use CountVectorizer or tfidfvectorizer for building a word matrix. For me countvectorizer gave better performance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer #Can use tfidffvectorizer as well\n\ncvect_desc = CountVectorizer(stop_words='english', max_features=200)\nfull_sparse = cvect_desc.fit_transform(full_data.description_new)\n # Renaming words to avoid collisions with other feature names in the model\ncol_desc = ['desc_'+ i for i in cvect_desc.get_feature_names()] \ncount_vect_df = pd.DataFrame(full_sparse.todense(), columns=col_desc)\nfull_data = pd.concat([full_data.reset_index(),count_vect_df],axis=1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "full_data.info()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Running Cross Validation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train =(full_data[full_data.flag=='train'])\ntest =(full_data[full_data.flag=='test'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "labels = {'high':0, 'medium':1, 'low':2}\ntrain['interest_level'] = train.interest_level.apply(lambda x: labels[x])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "feat = train.drop(['interest_level','flag','listing_id','description','index','description_new'],axis=1).columns.values",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier  as GBM\nfrom sklearn.ensemble import RandomForestClassifier  as RF\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def run_mod(train_X, test_X,train_Y):\n    reg = GBM(max_features = 'auto',n_estimators=200,random_state=1)\n    reg.fit(train_X,train_Y)\n    pred = reg.predict_proba(test_X)\n    imp = reg.feature_importances_\n    return pred,imp",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def cross_val(train,feat,split):\n    cv_scores = []\n    importances = []\n    # Cross Validation preprocessing\n    train_X = train[feat]\n    train_Y = train['interest_level']\n\n    train_X = train_X.as_matrix()\n    train_Y = train_Y.as_matrix()\n\n    test_X = test[feat]\n    test_X = test_X.as_matrix()\n\n    kf = StratifiedKFold(n_splits=split, shuffle=True, random_state=1)\n    for dev_index, val_index in kf.split(train_X,train_Y):\n            train_X_X, test_X_X = train_X[dev_index,:], train_X[val_index,:]\n            train_Y_Y, test_Y_Y = train_Y[dev_index,], train_Y[val_index,]\n            pred,imp = run_mod(train_X_X, test_X_X,train_Y_Y)\n            cv_scores.append(log_loss(test_Y_Y, pred))\n            importances.append(imp)\n    return np.mean(cv_scores),importances\n#print np.average(importances,axis=0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cv_score,imp = cross_val(train,feat,3)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cv_score",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Lets chaeck the importance of words\nimportances = list(np.average(imp,axis=0))\nfeatures = cvect_desc.get_feature_names()\ndf = pd.DataFrame({'words':features,'imp':importances}).sort_values(by='imp',ascending=False).head(30)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(12,15))\nsns.barplot(y=df.words,x=df.imp)\n# Remember, these are stemmed words",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "* Well, the score is using description and an untuned GBM. But a tuned one dies on me in this kernal (though it has score of 0.71). \n* It would be great if someone can post what score they get using XGB.\n* I think it is a good start for someone just starting out with text data. Similar transformation can be done with column feature.\n* It would be great if someone can help me optimize the clean() function.\n\nThanks!",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}