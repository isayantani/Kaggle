{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## A proxy for $/sqft and the interest on 1/2-baths",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import StratifiedKFold\nimport itertools as itertools\nfrom sklearn.metrics import log_loss\n\ndef get_skf_indexes(df, target, kfold=4):\n    X = df.values\n    y = df[target].values\n    skf = StratifiedKFold(n_splits=4);\n    skf.get_n_splits(X, y);\n    indexes = [[],[]]\n    for train_index, test_index in skf.split(X, y):\n        indexes[0].append(train_index) # Training indexes\n        indexes[1].append(test_index) # test indexes\n    return indexes\n\n\ndef get_lr_perf(df_train, df_test, feature='__to_check', target='response', n_quantile=20):\n    results = {}\n    # Inputs\n    xtrain = df_train[feature].values.reshape(-1,1)\n    ytrain = df_train[target].values\n    xtest = df_test[feature].values.reshape(-1,1)\n    ytest = df_test[target].values\n    # Evaluation as a single feature\n    lr = LogisticRegression()\n    lr.fit(xtrain, ytrain);\n    yptrain = lr.predict_proba(xtrain)\n    yptest = lr.predict_proba(xtest)\n    results['train.num'] = np.round(log_loss(ytrain, yptrain), 6)\n    results['test.num'] = np.round(log_loss(ytest, yptest), 6)\n    # Evaluation as a categorical feature using quantile buckets\n    bins = np.unique(np.percentile(xtrain, np.arange(n_quantile, 100, n_quantile)))\n    xtrainq = np.digitize(xtrain, bins)\n    xtestq = np.digitize(xtest, bins)\n    lb = LabelBinarizer()\n    x1 = lb.fit_transform(xtrainq)\n    x2 = lb.transform(xtestq)\n    lr.fit(x1, ytrain);\n    yptrain = lr.predict_proba(x1)\n    yptest = lr.predict_proba(x2)\n    results['train.cat'] = np.round(log_loss(ytrain, yptrain), 6)\n    results['test.cat'] = np.round(log_loss(ytest, yptest), 6)\n    return results",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 1) Price/sqft proxy using price, and number of bedrooms/bathrooms:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_json('../input/train.json')\ndf['response'] = \"0\"\ndf.loc[df.interest_level=='medium', 'response'] = \"1\"\ndf.loc[df.interest_level=='high', 'response'] = \"2\"",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In this first approach the aim is to parametrize the set of parameters of the following equation that miniminize log-loss from an \"univariant\" point of view:\n\n$$\\frac{Price}{A + N_{bedrooms}\\vert _{C_1}^{C_2} + B·N_{bathrooms}\\vert _{D_1}^{D_2}}$$\n\n- A baseline value (A) is set to avoid 0 divisions and to set the relative weight of the number of rooms regarding to the price.\n- The relative weights of the number of bedrooms and bathrooms are adjusted using parameter B.\n- Bedrooms and bathrooms are clipped using C and D respectively.\n\nFor each set of parameters I'll output two performance measures using a stratified 4-fold CV approach:\n\n- log-loss of the logistic classifier using as input the computed feature ('train.num' and 'test.num')\n- Since most of the classifiers will use a \"tree-based\" method, I'll perform bucketization (5% percentiles) of the computed feature and dummification. The resulting set of 20 features will be used as the input for a logistic classifier.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Parameters to check\nAA = (0.1, 0.5, 1, 2)\nCC = ((0, 4), (0, 3), (1, 4), (1, 3), (0, 2))\nDD = ((0, 3), (0, 2), (1, 3), (1, 2))\nBB = (0, 0.25, 0.5, 1, 2)\n# Reduced set of parameters to run here\nAA = (0.5, 1, 2)\nCC = ((0, 4), (0, 3), (1, 4), (1, 3))\nDD = ((0, 3), (0, 2))\nBB = (0.25, 0.5, 1)\n# Stratified kfold\nidx_train, idx_test = get_skf_indexes(df, 'response', kfold=2) # kfold=4, set to 2 to quickly run here\n# Get results\nY = pd.DataFrame()\nfor iper, (i_train, i_test) in enumerate(zip(idx_train, idx_test)):\n    print(iper)\n    df_train = df.iloc[i_train, :].copy()\n    df_test = df.iloc[i_test, :].copy()\n    # For each parameter combination\n    for A, C, D, B in itertools.product(AA, CC, DD, BB):\n        df_train['__to_check'] = (df_train.price / (A + df_train.bedrooms.clip(C[0], C[1]) + B*df_train.bathrooms.clip(D[0], D[1]))).values\n        df_test['__to_check'] = (df_test.price / (A + df_test.bedrooms.clip(C[0], C[1]) + B*df_test.bathrooms.clip(D[0], D[1]))).values\n        results = get_lr_perf(df_train, df_test, feature='__to_check', target='response', n_quantile=20)\n        results.update({'fold': iper, 'params': {'A':A, 'B': B, 'C': C, 'D':D}})\n        Y =  Y.append(pd.DataFrame(pd.Series(results)).transpose())\nfor i in ['train.cat', 'train.num', 'test.cat', 'test.num']:\n    Y[i] = Y[i].astype(float)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "Y.sort_values('test.cat')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "From these results we can conclude than the best proxy for price/sqft using price, bedrooms and bathrooms is:\n$$\\frac{Price}{1 + N_{bedrooms}\\vert _{1}^{4} + 0.5·N_{bathrooms}\\vert _{0}^{2}}$$",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 2) Uninteresting half bathrooms\n\nYou'll have noticed that half bathrooms show mean interests far above the mean interest level.\n\nLet's compute a boolean feature for half-bathrooms and clip the number of bathrooms to [0, 4]:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df['half_bathrooms'] = ((np.round(df.bathrooms) - df.bathrooms)!=0).astype(float) # Half bathrooms? 1.5, 2.5, 3.5...\ndf['bathrooms'] = df.bathrooms.clip(0,4) # Reduce outlier effects",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's demonstrate the half bathrooms unininterest from a statistical point of view. We'll fit two models with and without using the half bathrooms boolean variable. I'll use a likelihood ratio test to demonstrate that the model including the feature has better fit:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Build two models with and without 'half_bathrooms' feature\nformula1 = 'response ~ bathrooms'\nformula2 = 'response ~ bathrooms + half_bathrooms'\nmodel1 = smf.glm(formula=formula1, data=df, family=sm.families.Binomial())\nmodel2 = smf.glm(formula=formula2, data=df, family=sm.families.Binomial())\nresult1 = model1.fit()\nresult2 = model2.fit()\n# Likelihood ratio test\nllf_1 = result1.llf\nllf_2 = result2.llf\ndf_1 = result1.df_resid \ndf_2 = result2.df_resid \nlrdf = (df_1 - df_2)\nlrstat = -2*(llf_1 - llf_2)\nlr_pvalue = stats.chi2.sf(lrstat, df=lrdf)\n# Print results\nprint(formula1)\nprint(result1.summary())\nprint(formula2)\nprint(result2.summary())\nprint('Likelihood ratio test', lr_pvalue)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "These results can also be noticed by using a barplot showing the interest frequencies depending on the number of bathrooms:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "x = pd.crosstab(df.bathrooms, df.interest_level)[['low', 'medium', 'high']]\nx.div(x.sum(1), 0).plot(kind='bar', color=['red', 'yellow', 'green'], stacked=True);",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}